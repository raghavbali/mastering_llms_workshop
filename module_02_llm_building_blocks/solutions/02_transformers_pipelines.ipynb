{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ab9deb-d5cd-45d4-a4be-4748d15df4e5",
   "metadata": {},
   "source": [
    "# Transformer Task Pipelines \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop/blob/main/module_01/solutions/03_explore_transformers.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc0ff9d-1083-484d-9f48-9883beaa48f3",
   "metadata": {},
   "source": [
    "## BERT-ology\n",
    "- BERT, or __[Bi-Directional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805)__, was presented by Devlin et al., a team at Google AI in 2018\n",
    "- Multi-task Learning: BERT also helped push the transfer-learning envelope in the NLP domain by showcasing how a pre-trained model can be fine-tuned for various tasks to provide state-of-the-art performance\n",
    "- BERT tweaked the usual Language Model objective to only predict next token based on past context by building context from both directions, i.e. the objective of predicting masked words along with next sentence prediction.\n",
    "\n",
    "\n",
    "<img src=\"../../assets/02_bert_models_layout_notebook_3.jpeg\">\n",
    "\n",
    "> source [PLM Papers](https://github.com/thunlp/PLMpapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a38e20-e963-4d1a-b65c-8e09a60a267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb245c9f-e147-49ef-a69c-e3defa41fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define some configs/constants\n",
    "DISTILBET_BASE_UNCASED_CHECKPOINT = \"distilbert/distilbert-base-uncased\"\n",
    "DISTILBET_QA_CHECKPOINT = \"distilbert/distilbert-base-uncased-distilled-squad\"\n",
    "DISTILBET_CLASSIFICATION_CHECKPOINT = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a88cc2e-b8fb-48f9-8537-015420b4d902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend Accelerator Device=mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    DEVICE_ID = 0\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    Tensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor\n",
    "    DEVICE_ID = 0\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    Tensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor\n",
    "    DEVICE_ID = -1\n",
    "print(f\"Backend Accelerator Device={DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f241206-25ed-4021-9457-e1bdd665a029",
   "metadata": {},
   "source": [
    "### Predicting the Masked Token\n",
    "This was a unique objective when BERT was originally introduced as compared to usual NLP tasks such as classification. The objective requires us to prepare a dataset where we mask a certain percentage of input tokens and train the model to learn to predict those tokens. This objective turns out to be very effective in helping the model learn the nuances of language. \n",
    "\n",
    "In this first task we will test the pre-trained model against this objective itself. The model outputs a bunch of things such as the predicted token, encoded index of the predicted token/word along with a score which indicates the model's confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e3efdda-c31f-438e-a19e-4593570dc323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghavbali/.pyenv/versions/3.11.9/envs/datahack/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6093319058418274,\n",
       "  'token': 3007,\n",
       "  'token_str': 'capital',\n",
       "  'sequence': 'bangalore is the it capital of india.'},\n",
       " {'score': 0.11960869282484055,\n",
       "  'token': 9594,\n",
       "  'token_str': 'hub',\n",
       "  'sequence': 'bangalore is the it hub of india.'},\n",
       " {'score': 0.07699015736579895,\n",
       "  'token': 2110,\n",
       "  'token_str': 'state',\n",
       "  'sequence': 'bangalore is the it state of india.'},\n",
       " {'score': 0.018147675320506096,\n",
       "  'token': 11909,\n",
       "  'token_str': 'gateway',\n",
       "  'sequence': 'bangalore is the it gateway of india.'},\n",
       " {'score': 0.014192461967468262,\n",
       "  'token': 4075,\n",
       "  'token_str': 'headquarters',\n",
       "  'sequence': 'bangalore is the it headquarters of india.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_pipeline = pipeline(\n",
    "    'fill-mask',\n",
    "    model=DISTILBET_BASE_UNCASED_CHECKPOINT,\n",
    "    device=DEVICE_ID\n",
    ")\n",
    "mlm_pipeline(\"Bangalore is the IT [MASK] of India.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b163d-cc5a-4619-9975-09a572489026",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "This is an interesting NLP task and quite complex one as well. For this task, the model is provided input consisting of the context along with a question and it predicts the answer by selecting text from the context. The training setup for this task is a bit involved process, the following is an overview:\n",
    "- The training input as triplet of context, question and answer\n",
    "- This is transformed as combined input of the form ``[CLS]question[SEP]context[SEP]`` or ``[CLS]contex[SEP]question[SEP]`` with answer acting as the label\n",
    "- The model is trained to predict the start and end indices of the the corresponding answer for each input.\n",
    "\n",
    "\n",
    "For our current setting, we will leverage both _pretrained_ and _fine-tuned_ versions of **DistilBERT** via the _question-answering_ pipeline and understand the performance difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ae5a96-fd84-4b11-9943-5dfac261c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "qa_ft_pipeline = pipeline(\n",
    "    'question-answering',\n",
    "    model=DISTILBET_QA_CHECKPOINT,\n",
    "    device=DEVICE_ID\n",
    ")\n",
    "qa_pt_pipeline = pipeline(\n",
    "    'question-answering',\n",
    "    model=DISTILBET_BASE_UNCASED_CHECKPOINT, #TODO: Set the pretrained \n",
    "    device=DEVICE_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cde485cf-1c21-4f34-a2d2-4d0130854905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a snippet about BERT like models from the module itself\n",
    "context = \"\"\"The key contribution from this set of models is the masked language modeling objective during the pre-training phase, where some tokens in the input are masked, and the model is trained to predict them (we will cover these in the upcoming section). Key works in this group of architectures are BERT, RoBERTa (or optimized BERT), DistilBERT (lighter and more efficient BERT), ELECTRA and ALBERT.\n",
    "In this notebook we will work through the task of Question Answering where our language model will learn to answer questions based on the context provided.\"\"\"\n",
    "question = \"What are the key works in this set of models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf038b7-e8c8-425e-bbd4-026895e73d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_qa_result= qa_ft_pipeline(\n",
    "    question=question,\n",
    "    context=context\n",
    ")\n",
    "\n",
    "pt_qa_result= qa_pt_pipeline(\n",
    "    question=question,\n",
    "    context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cd01d95-491a-4c22-afac-833de30b17cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************\n",
      "Context:The key contribution from this set of models is the masked language modeling objective during the pre-training phase, where some tokens in the input are masked, and the model is trained to predict them (we will cover these in the upcoming section). Key works in this group of architectures are BERT, RoBERTa (or optimized BERT), DistilBERT (lighter and more efficient BERT), ELECTRA and ALBERT.\n",
      "In this notebook we will work through the task of Question Answering where our language model will learn to answer questions based on the context provided.\n",
      "*******************************************************\n",
      "Question:What are the key works in this set of models?\n",
      "-------------------------------------------------------\n",
      "Response from Fine-Tuned Model:\n",
      "{'score': 0.010789199732244015, 'start': 294, 'end': 326, 'answer': 'BERT, RoBERTa (or optimized BERT'}\n",
      "\n",
      "Response from Pretrained Model:\n",
      "{'score': 0.0001713668170850724, 'start': 349, 'end': 394, 'answer': 'and more efficient BERT), ELECTRA and ALBERT.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"*\"*55)\n",
    "print(f\"Context:{context}\")\n",
    "print(\"*\"*55)\n",
    "print(f\"Question:{question}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"Response from Fine-Tuned Model:\\n{ft_qa_result}\")\n",
    "print()\n",
    "print(f\"Response from Pretrained Model:\\n{pt_qa_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0ed16-e828-4ede-892e-04622bf82a35",
   "metadata": {},
   "source": [
    "# Generative Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542feb8-bff8-424d-8f9a-f3a38420ee16",
   "metadata": {},
   "source": [
    "## Behold, its GPT (Generative pre-training)\n",
    "\n",
    "The first model in this series is called GPT, or Generative Pre-Training. It was released in [2018](https://openai.com/blog/language-unsupervised/), about the same time as the BERT model. The paper presents a task-agnostic architecture based on the ideas of transformers and unsupervised learning.\n",
    "\n",
    "- GPT is essentially a language model based on the __transformer-decoder__ \n",
    "- Introduction of large training datasets: __BookCorpus__ dataset contains over 7,000 unique, unpublished books across different genres\n",
    "- The GPT architecture makes use of 12 decoder blocks (as opposed to 6 in the original transformer) with 768-dimensional states and 12 self-attention heads each.\n",
    "\n",
    "\n",
    "### GPT-2\n",
    "- Radford et al. presented the GPT-2 model as part of their work titled [Language Models are Unsupervised Multi-task Learners in 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- The model achieves state-of-the-art performance in a few-shot setting\n",
    "- Similar to GPT, the secret sauce for GPT-2 is its dataset. The authors prepared a massive 40 GB dataset by crawling 45 million outbound links from a social networking site called Reddit.\n",
    "- The vocabulary was also expanded to cover 50,000 words and the context window was expanded to 1,024 tokens (as compared to 512 for GPT).\n",
    "\n",
    "\n",
    "### GPT-3\n",
    "- OpenAI published paper titled [Language Models are Few Shot Learners](https://arxiv.org/abs/2005.14165) in May 2020. \n",
    "- This paper introduces the mammoth __175 billion-parameter GPT-3 model__.\n",
    "- Apart from more layers and parameters, this model made use of sparse attention\n",
    "- Dataset again played a key role, a 300 billion-token dataset based on existing datasets like Common Crawl (filtered for better content), WebText2 (a larger version of WebText used for GPT-2), Books1 and Books2, and the Wikipedia dataset was prepared for this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd9768-520a-462f-9b85-1d57f5ab44e7",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "By far the most widely used application from the NLP world is language modeling. We use it daily on our phone keyboards, email applications and a ton of other places.\n",
    "\n",
    "In simple words, a language model takes certain text as input context to generate the next set of words as output. This is interesting because a language model tries to understand the input context, the language structure (though in a very naive way) to predict the next word(s). We use it in the form of text completion utilities on search engines, chat platforms, emails etc. all the time. Language models are a perfect real life application of NLP and showcase the power of RNNs.\n",
    "\n",
    "Language models can be developed train in different ways. The most common and widely used method is the sliding window approach. The model takes a small window of text as input and tried to predict the next word as the output. The following figure illustrates the same visually.\n",
    "\n",
    "<img src=\"../../assets/02_lm_training_notebook_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc37334-041b-475b-b281-2b6c42902b27",
   "metadata": {},
   "source": [
    "### PreTrained GPT2 for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f722ddfd-4fe3-4aeb-afbd-1eb3f964e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea73c89c-646f-47c7-8f36-ef4bc7b5b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend Accelerator Device=cpu\n"
     ]
    }
   ],
   "source": [
    "# generative tasks are not available through MPS/Apple Silicon\n",
    "DEVICE = 'cpu'\n",
    "Tensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "DEVICE_ID = -1\n",
    "print(f\"Backend Accelerator Device={DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6949423-7182-4435-988c-89ed678e8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghavbali/.pyenv/versions/3.11.9/envs/datahack/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") #TODO: get pretrained GPT2 tokenizer\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b909212-f6a3-46fd-8acc-5dc77224bead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The king of England is a man of great wealth and power, and he is a man of great wealth and power. He is a man of great wealth and power. He is a man of great wealth and power. He\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "model_inputs = tokenizer('The king of England is', return_tensors='pt').to(DEVICE)\n",
    "\n",
    "# generate 40 new tokens\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48210990-c4d2-4103-91ab-184addedba15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
