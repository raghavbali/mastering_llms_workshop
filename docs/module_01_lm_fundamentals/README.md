# Text Representation and Contextual Embeddings
This module covers the fundamentals of text representation and contextual embeddings using transformers.

## Table of Contents
- [Text Representation Notebook](01_text_representation.ipynb)
- [Contextual Embeddings Notebook](02_contextual_embeddings.ipynb)

> Above content was auto generated by devstral @continue in agentic mode

-------------------------
# Language Model Fundamentals

This module introduces the foundational concepts of representing and embedding text for language models. It covers both traditional and modern approaches for converting raw text into machine-understandable vectors, starting from basic text representations to advanced contextual embeddings using state-of-the-art transformer models.

## Table of Contents

- [Text Representation: From Tokens to Embeddings](01_text_representation.ipynb)
- [Contextual Embeddings: Understanding Words in Context](02_contextual_embeddings.ipynb)

---

### Text Representation: From Tokens to Embeddings

[01_text_representation.ipynb](01_text_representation.ipynb)  
Learn about the importance of feature engineering for text data, including various tokenization strategies (character-level, word-level), vectorization techniques such as Bag of Words and TF-IDF, and powerful word embedding models like Word2Vec and FastText. Practical implementations are provided using real-world text datasets and visualizations of word embeddings.

### Contextual Embeddings: Understanding Words in Context

[02_contextual_embeddings.ipynb](02_contextual_embeddings.ipynb)  
Delve into how modern language models generate embeddings that capture word meaning based on surrounding context, overcoming the limitations of static embeddings. The notebook demonstrates the use of transformer-based models (e.g., BERT, MiniLM) for producing contextual embeddings and explores their applications in tasks like semantic search, showing how the same word can have different representations depending on its usage.

> Above content was auto generated by devstral @continue in agentic mode
---

> The below content was auto generated by Github CoPilot
> See how it follows instructions within the copilot-instructions.md file
> but goes a bit overboard!
> More details on [PR#4](https://github.com/raghavbali/mastering_llms_workshop_dhs2025/pull/4)

# Module 01: Language Model Fundamentals

This module introduces the foundational concepts of text representation and embeddings that form the building blocks of modern language models. You'll learn how to convert raw text into numerical representations that machines can understand and process, progressing from traditional methods to state-of-the-art contextual embeddings.

## Table of Contents

- [Text Representation: From Tokens to Embeddings](./01_text_representation.ipynb) - Learn tokenization, vectorization, and word embeddings including Word2Vec and FastText
- [Contextual Embeddings: Understanding Words in Context](./02_contextual_embeddings.ipynb) - Explore transformer-based embeddings that capture meaning based on context

## Learning Objectives

By the end of this module, you will:

- Understand the importance of text representation in NLP and language modeling
- Master different tokenization strategies (character-level and word-level)
- Implement traditional text vectorization techniques like Bag of Words and TF-IDF
- Build and train word embedding models using Word2Vec and FastText
- Visualize word embeddings to understand semantic relationships
- Handle out-of-vocabulary words using subword representations
- Explore contextual embeddings from transformer models
- Compare static vs. contextual word representations

## Module Content

### Text Representation: From Tokens to Embeddings
**[01_text_representation.ipynb](./01_text_representation.ipynb)**

This comprehensive notebook covers the fundamentals of converting text into numerical representations:

- **Text Preprocessing & Tokenization**: Learn to break down text using torchtext tokenizers
- **Vocabulary Building**: Create word-to-index mappings and handle vocabulary constraints
- **Word2Vec Embeddings**: Train skip-gram models to capture semantic word relationships
- **FastText Integration**: Handle out-of-vocabulary words using subword information
- **Embedding Visualization**: Use t-SNE to visualize word embeddings in 2D space
- **Practical Applications**: Work with real text data from "War of the Worlds" by H.G. Wells

Key concepts demonstrated include tokenization strategies, vocabulary construction, embedding training, similarity analysis, and visualization techniques.

### Contextual Embeddings: Understanding Words in Context
**[02_contextual_embeddings.ipynb](./02_contextual_embeddings.ipynb)**

Explore how modern language models generate context-aware word representations:

- **Contextual vs. Static Embeddings**: Understand the limitations of static word embeddings
- **Transformer-based Models**: Work with pre-trained models like BERT and MiniLM
- **Dynamic Word Representations**: See how the same word gets different embeddings based on context
- **Semantic Search Applications**: Implement practical use cases for contextual embeddings
- **Model Comparison**: Compare different transformer architectures and their embedding capabilities

This notebook demonstrates the evolution from static to dynamic word representations and their practical applications in modern NLP tasks.

## Prerequisites

- Basic Python programming knowledge
- Understanding of NumPy and basic linear algebra
- Familiarity with Jupyter notebooks
- Basic knowledge of machine learning concepts

## Dependencies

The notebooks use the following key libraries:
- `torch` and `torchtext` for tokenization and tensor operations
- `gensim` for Word2Vec and embedding models
- `fasttext` for subword representations
- `transformers` for contextual embeddings
- `matplotlib` and `sklearn` for visualization
- `pandas` and `numpy` for data manipulation

## Dataset

This module uses "The War of the Worlds" by H.G. Wells as the primary text corpus (`book_corpus.txt`), providing a rich literary dataset for exploring text representation techniques and training embedding models.

> Generated by Claude 3.5 Sonnet on December 19, 2024

---