{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1b1298",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop/blob/main/docs/module_04_llm_apps/02_dspy_demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ed0667c-fe9b-45b4-83b3-2ad4ead66225",
   "metadata": {},
   "source": [
    "# <img src=\"../assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
    "---\n",
    "<img src=\"../assets/dspy_banner.png\">\n",
    "\n",
    "---\n",
    "\n",
    "## Intro\n",
    "- Language Models are like extremely complex machines with capabilities to retrieve and reformulate information from an **extremely large latent space**.\n",
    "- To guide this search and achieve desired responses we heavily rely on **complex, long and brittle prompts** which (at times) are very specific to certain LLMs\n",
    "- Being an open area of research, teams are working from different perspectives to abstract and enable rapid development of **LLM-enabled systems**.\n",
    "- **StanfordDSpy** is one such framework for algorithmally optimizing LM prompts and weights.\n",
    "\n",
    "\n",
    "> $_{DSpy\\ logo\\ is\\ copyright/ownership\\ of\\ respective\\ teams}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39a983-e508-43a3-aaed-7c388a0be75b",
   "metadata": {},
   "source": [
    "## Ok, You Got Me Intrigued, Tell Me More?\n",
    "\n",
    "- The DSpy framework takes inspiration from deep learning frameworks such as <img src=\"./assets/pytorch_logo.png\" width=\"2%\">[PyTorch](https://pytorch.org/)\n",
    "    - For instance, to build a deep neural network using PyTorch we simply use standard layers such as ``convolution``, ``dropout``, ``linear`` and attach them to optimizers like ``Adam`` and train without worrying about implementing these from scratch everytime.\n",
    "- Similarly, DSpy provides a a set of standard general purpose **modules** (``ChainOfThought``,``Predict``), **optimizers** (``BootstrapFewShotWithRandomSearch``) and helps us build systems by composing these components as layers into a ``Program`` without explicitly dealing with prompts! Neat isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf9d1b7-8030-433a-a830-3f506cabc010",
   "metadata": {},
   "source": [
    "### Usual Prompt Based Workflow\n",
    "<img src=\"../assets/prompt_workflow.png\" width=\"75%\">\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e534227-f2b7-4acb-a7e9-91c47e33f769",
   "metadata": {},
   "source": [
    "### LangChain-Like Workflow\n",
    "\n",
    "<img src=\"../assets/langchain_workflow.png\" width=\"75%\">\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f045e-8319-433f-bbe1-ff858bcd4d73",
   "metadata": {},
   "source": [
    "### DSpy Workflow\n",
    "\n",
    "<img src=\"../assets/dspy_workflow.png\" >\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2a7ff-28f1-4cdc-a35a-0b63e7d4cb1b",
   "metadata": {},
   "source": [
    "## Time to Put Words into Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c26b86-fe6b-434f-88e4-23d73ca416d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install dspy==2.6.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41c74d8-0414-42c9-be3a-58c63bac0035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.27'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "dspy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3501ddfa-233e-4b74-95a8-9da46ea63e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import dspy\n",
    "from dsp.utils import deduplicate\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from scraper_utils import NB_Markdown_Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48faf88-dabe-4c97-95d8-7ab6cd467cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_TOKEN = '<YOUR TOKEN>'\n",
    "lm = dspy.LM('openai/gpt-4o-2024-11-20', api_key=OPENAI_TOKEN)\n",
    "# lm = dspy.LM(\n",
    "#     'ollama_chat/llama3.1',\n",
    "#     api_base='http://localhost:11434',\n",
    "#     api_key=OPENAI_TOKEN\n",
    "# )\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a20e56f-4061-4217-8db6-631d55ade272",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "We will scrape and extract text/markdown cells from all notebooks in this repository and prepare a dataset using the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c7fbb42-fbb1-484c-8f40-932d367b80f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../module_01_lm_fundamentals',\n",
       " '../module_03_instruction_tuning_and_alignment',\n",
       " '../module_02_llm_building_blocks',\n",
       " '../module_04_llm_apps']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'../{d}' for d in os.listdir(\"../\") if d.startswith(\"module\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59bd1bea-e566-4cac-8238-42713c59dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_scraper = NB_Markdown_Scraper([f'../{d}' for d in os.listdir(\"../\") if d.startswith(\"module\")])\n",
    "nb_scraper.scrape_markdowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c8b266-e915-46aa-bfd0-a0af44f5d64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['module_01_lm_fundamentals_01_text_representation', 'module_01_lm_fundamentals_02_contextual_embeddings', 'module_03_instruction_tuning_and_alignment_01_instruction_tuning_llama_txt2py', 'module_03_instruction_tuning_and_alignment_02_RLHF_phi2', 'module_03_instruction_tuning_and_alignment_03_zephyr_alignment_dpo', 'module_02_llm_building_blocks_02_transformers_pipelines', 'module_02_llm_building_blocks_03_training_language_models', 'module_02_llm_building_blocks_01_transformers', 'module_02_llm_building_blocks_04_llm_training_and_scaling', 'module_04_llm_apps_03_mcp_getting_started', 'module_04_llm_apps_01_retrieval_augmented_llm_app', 'module_04_llm_apps_02_dspy_demo'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_scraper.notebook_md_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b5e711-ac90-4ece-bdd8-6905d9f2a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dspy_content.tsv\", \"w\") as record_file:\n",
    "    for k,v in nb_scraper.notebook_md_dict.items():\n",
    "        record_file.write(f\"{k}\\t{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53358b2d-7930-449e-9008-95a8f180b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = []\n",
    "ctr = 1\n",
    "for k,_ in nb_scraper.notebook_md_dict.items():\n",
    "    doc_ids.append(f'{ctr}_{k}')\n",
    "    ctr+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef35ce5-13b9-494a-82e0-16195bd12099",
   "metadata": {},
   "source": [
    "## Setup Chroma\n",
    "We started this workshop with **text representation** as one of the key components of any NLP system.\n",
    "As we progressed from simple Bag of Words setup to highly contextualised Transformer models, we now have rich & dense representations.\n",
    "The utility of such representations also increased multifold from word/sentence representations to features that can used for a number of downstream tasks.\n",
    "\n",
    "These representations, also called as vectors or embedding vectors are long series of numbers. Their retrieval and persistence requires specialised database management systems called **Vector Databases**.\n",
    "\n",
    "Vector Databases are particularly suited for handling data in the form of vectors, embeddings, or feature representations, which are commonly used in various applications like machine learning, natural language processing, computer vision, and recommendation systems.\n",
    "\n",
    "Key Features:\n",
    "- High-dimensional Data Support\n",
    "- Similarity Search\n",
    "- Indexing Techniques\n",
    "- Dimensionality Reduction\n",
    "\n",
    "There are a number of different off-the-shelf options available, such as:\n",
    "- [ChromaDB](https://www.trychroma.com/)\n",
    "- [PineCone](https://www.pinecone.io/)\n",
    "- [Milvus](https://milvus.io/)\n",
    "- [Weaviate](https://weaviate.io/)\n",
    "- [AeroSpike](https://aerospike.com/)\n",
    "- [OpenSearch](https://opensearch.org/)\n",
    "\n",
    "**Let's Install the Dependencies**\n",
    "```python\n",
    "!pip install -q chromadb\n",
    "!pip install retry\n",
    "!pip install -U sentence-transformers\n",
    "```\n",
    "\n",
    "> ensure chroma is running on your terminal `$>chroma run --path ./chromadb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65251496-7177-42a4-976c-b5314bcdc02c",
   "metadata": {},
   "source": [
    "## Vector Database: ChromaDB\n",
    "\n",
    "As mentioned above, there are a number of offering available. For this workshop we will make use of\n",
    "[ChromaDB](https://www.trychroma.com/).\n",
    "\n",
    "It is a super simple setup which is easy to use. The following figure showcases the overall flow\n",
    "\n",
    "<img src=\"../assets/04_chroma_workflow.png\">\n",
    "\n",
    "> Source :[chromadb](https://docs.trychroma.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46476bc4-0aa4-401c-bc11-951a7985b287",
   "metadata": {},
   "source": [
    "## Sentence Transformers\n",
    "\n",
    "This is an amazing python framework initially proposed along with the seminal paper titled [Sentence-BERT](https://www.sbert.net/).\n",
    "It provides clean high-level interfaces to easily use Language Models for computing text embeddings for various use-cases.\n",
    "\n",
    "In this notebook we will leverage pretrained models supported by sentence transformer rather than directly using the package.\n",
    "\n",
    "There is a [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) now maintained to keep track of the state-of-the-art embedding models called the **Massive Text Embedding Benchmark (MTEB) Leaderboard**\n",
    "\n",
    "<img src=\"../assets/04_mteb.png\">\n",
    "\n",
    "> Source : [HuggingFace](https://huggingface.co/spaces/mteb/leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a1d418-aa47-4ff2-bc75-ba1f8318095b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghavbali/.pyenv/versions/3.11.9/envs/datahack/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "chroma_emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "client = chromadb.HttpClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c70cddc6-f946-454b-b7a4-6ff5c64b82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_COLLECTION_NAME = \"workshop_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a26fb8bb-0fe8-4701-84f8-d7a23fcf71f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.delete_collection(CHROMA_COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "716310d4-9f55-41f3-a6b9-f509c7680c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\n",
    "    CHROMA_COLLECTION_NAME,\n",
    "    embedding_function=chroma_emb_fn,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76e95f64-c238-4a86-94b2-d3f040346d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to collection\n",
    "collection.add(\n",
    "    documents=[v for _,v in nb_scraper.notebook_md_dict.items()], \n",
    "    ids=doc_ids, # must be unique for each doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2443ccd3-7d06-4071-9657-408cba6d4fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1_module_01_lm_fundamentals_01_text_representation', '4_module_03_instruction_tuning_and_alignment_02_RLHF_phi2', '2_module_01_lm_fundamentals_02_contextual_embeddings']\n",
      "[0.799641268192753, 0.8011068851342594, 0.851910180221277]\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"RLHF\"], # Chroma will embed using the function we provided\n",
    "    n_results=3 # how many results to return\n",
    ")\n",
    "print(results['ids'][0])\n",
    "print(results['distances'][0])\n",
    "#print([i[:100] for j in results['documents'] for i in j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c036bf-6144-4945-9732-0ddebd2426f4",
   "metadata": {},
   "source": [
    "# Chroma as RM for DSPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15544929-dbd3-4598-9c60-a193be2c7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5fcb6f8-f674-44c1-ab1d-3d8c489c2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "import os\n",
    "import openai\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "475245a5-37ea-4e60-8bda-60c10f0c490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id::1_module_01_lm_fundamentals_01_text_representation\n",
      "Document score::0.799641268192753\n",
      "Document: <a target=\"_blank\" href=\"https://colab.research.go ... \n",
      "\n",
      "Document id::4_module_03_instruction_tuning_and_alignment_02_RLHF_phi2\n",
      "Document score::0.8011068851342594\n",
      "Document: <a target=\"_blank\" href=\"https://colab.research.go ... \n",
      "\n",
      "Document id::2_module_01_lm_fundamentals_02_contextual_embeddings\n",
      "Document score::0.851910180221277\n",
      "Document: <a target=\"_blank\" href=\"https://colab.research.go ... \n",
      "\n",
      "Document id::5_module_03_instruction_tuning_and_alignment_03_zephyr_alignment_dpo\n",
      "Document score::0.8664296651969585\n",
      "Document: <a target=\"_blank\" href=\"https://colab.research.go ... \n",
      "\n",
      "Document id::7_module_02_llm_building_blocks_03_training_language_models\n",
      "Document score::0.9139603656846649\n",
      "Document: <a target=\"_blank\" href=\"https://colab.research.go ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever_model = ChromadbRM(\n",
    "    CHROMA_COLLECTION_NAME,\n",
    "    './chromadb/',\n",
    "    embedding_function=chroma_emb_fn,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "results = retriever_model(\"RLHF\", k=5)\n",
    "\n",
    "for result in results:\n",
    "    print(f'Document id::{result.id}')\n",
    "    print(f'Document score::{result.score}')\n",
    "    print(\"Document:\", result.long_text[:50],'...' ,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb257f26-83be-492a-bd2f-0f2fb6803895",
   "metadata": {},
   "source": [
    "## Basic DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e669fa46-374c-400c-bd62-d29165822918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LM and RM\n",
    "dspy.settings.configure(\n",
    "    lm=lm,\n",
    "    rm=retriever_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c738473d-a729-4e7c-99a7-1736bd3ecffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54c4a40f-a3c6-4cf5-8129-be079ba4e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deb768da-94f8-4c47-81ce-191e476daab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List the models covered in module02\n",
      "Predicted Answer: GPT-2\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    }
   ],
   "source": [
    "my_question = \"List the models covered in module02\"\n",
    "compiled_rag = RAG()\n",
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "pred = compiled_rag(my_question)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "for c in pred.context:\n",
    "    print(f\"Retrieved Contexts (truncated):{c[:100]}...\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529bb6a-e39f-4a07-b0f0-33c53f478f61",
   "metadata": {},
   "source": [
    "## Multi-Hop DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f7f79f4-b908-4e76-833b-4da612550ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1261053-1212-476d-81f2-6eb4329fee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateSearchQuery(dspy.Signature):\n",
    "    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    query = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b650da3-a6fa-4a9b-869b-8d07ae96013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBaleen(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_query[hop](context=context, question=question).query\n",
    "            passages = self.retrieve(query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        pred = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=pred.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f13c28c6-1e04-47dd-a6f5-25c6581c134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List the models covered in module02\n",
      "Predicted Answer: GPT-2\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    }
   ],
   "source": [
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "uncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program\n",
    "pred = uncompiled_baleen(my_question)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "for c in pred.context:\n",
    "    print(f\"Retrieved Contexts (truncated):{c[:100]}...\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc408a11-7281-4e62-9eea-c230c1b8e4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-17T23:24:37.314904]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `context` (str): may contain relevant facts\n",
      "2. `question` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `answer` (str): often between 1 and 5 words\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## context ## ]]\n",
      "{context}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Answer questions with short factoid answers.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «««\n",
      "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop_dhs2025/blob/main/docs/module_02_llm_building_blocks/03_training_language_models.ipynb\">\n",
      "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
      "    </a> # Training Language Models ---\n",
      "    ## Recap\n",
      "    - **Training Paradigm**: The notebook introduces the two-step training paradigm for transformer models, involving pretraining on large datasets to learn general language patterns and fine-tuning on task-specific data.\n",
      "    - **Dataset Preparation and Tokenization**: It details how to prepare datasets for training, including loading datasets, tokenizing text using a tokenizer from Hugging Face, and mapping tokenized data for model input.\n",
      "    - **Model Training and Generation**: The notebook covers setting up the training environment, configuring the GPT-2 model, and running the training process. Additionally, it demonstrates code generation using the trained model with different temperature settings. ## The 2-Step Training Paradigm\n",
      "    \n",
      "    Transformers are complex models built like LEGO blocks using multiple smart and specialized components.\n",
      "    - A vanilla transformer model consists of separate stacks of encoders and decoders.\n",
      "    - Each encoder block includes multi-head self-attention, enabling the model to capture relationships between tokens regardless of their positions.\n",
      "    - Residual connections help maintain gradient flow, preventing the vanishing gradient problem.\n",
      "    - Layer normalization ensures training stability, and feed-forward layers introduce non-linearity and learn complex token interactions.\n",
      "    - Decoder blocks contain the same components but also include an encoder-decoder attention mechanism to incorporate context from the encoder.\n",
      "    - The model uses embedding layers to convert tokens into a continuous latent space for contextual learning and positional encoding to preserve the order of tokens in the sequence\n",
      "    \n",
      "    <img src=\"../assets/02_training_setup_01.png\">\n",
      "    \n",
      "    > Source: Backcock, Bali et. al.\n",
      "    \n",
      "    The two-step **training paradigm** in transformer models is designed as follows:\n",
      "    - **Pretraining** on large raw datasets like open-webtext, allowing the model to learn broad language patterns and concepts. This forms a strong foundation for various NLP tasks.\n",
      "    - The second step, **fine-tuning**, uses task-specific datasets to tailor the model to particular tasks or domains. ## Imports and Utils ## Dataset Preparation ## Tokenize ## Load Model ## Training Setup ## Push to Hub ## Let's Generate Some Code\n",
      "»»»\n",
      "[2] «««\n",
      "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop/blob/main/docs/module_04_llm_apps/02_dspy_demo.ipynb\">\n",
      "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
      "    </a> # <img src=\"../assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
      "    ---\n",
      "    <img src=\"../assets/dspy_banner.png\">\n",
      "    \n",
      "    ---\n",
      "    \n",
      "    ## Intro\n",
      "    - Language Models are like extremely complex machines with capabilities to retrieve and reformulate information from an **extremely large latent space**.\n",
      "    - To guide this search and achieve desired responses we heavily rely on **complex, long and brittle prompts** which (at times) are very specific to certain LLMs\n",
      "    - Being an open area of research, teams are working from different perspectives to abstract and enable rapid development of **LLM-enabled systems**.\n",
      "    - **StanfordDSpy** is one such framework for algorithmally optimizing LM prompts and weights.\n",
      "    \n",
      "    \n",
      "    > $_{DSpy\\ logo\\ is\\ copyright/ownership\\ of\\ respective\\ teams}$ ## Sentence Transformers\n",
      "    \n",
      "    This is an amazing python framework initially proposed along with the seminal paper titled [Sentence-BERT](https://www.sbert.net/).\n",
      "    It provides clean high-level interfaces to easily use Language Models for computing text embeddings for various use-cases.\n",
      "    \n",
      "    In this notebook we will leverage pretrained models supported by sentence transformer rather than directly using the package.\n",
      "    \n",
      "    There is a [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) now maintained to keep track of the state-of-the-art embedding models called the **Massive Text Embedding Benchmark (MTEB) Leaderboard**\n",
      "    \n",
      "    <img src=\"../assets/04_mteb.png\">\n",
      "    \n",
      "    > Source : [HuggingFace](https://huggingface.co/spaces/mteb/leaderboard) # Chroma as RM for DSPY ## Basic DSPy Program ## Multi-Hop DSPy Program ## How Does it All Go? ## Ok, You Got Me Intrigued, Tell Me More?\n",
      "    \n",
      "    - The DSpy framework takes inspiration from deep learning frameworks such as <img src=\"./assets/pytorch_logo.png\" width=\"2%\">[PyTorch](https://pytorch.org/)\n",
      "        - For instance, to build a deep neural network using PyTorch we simply use standard layers such as ``convolution``, ``dropout``, ``linear`` and attach them to optimizers like ``Adam`` and train without worrying about implementing these from scratch everytime.\n",
      "    - Similarly, DSpy provides a a set of standard general purpose **modules** (``ChainOfThought``,``Predict``), **optimizers** (``BootstrapFewShotWithRandomSearch``) and helps us build systems by composing these components as layers into a ``Program`` without explicitly dealing with prompts! Neat isn't it? ### Usual Prompt Based Workflow\n",
      "    <img src=\"../assets/prompt_workflow.png\" width=\"75%\">\n",
      "    \n",
      "    ---\n",
      "     ### LangChain-Like Workflow\n",
      "    \n",
      "    <img src=\"../assets/langchain_workflow.png\" width=\"75%\">\n",
      "    \n",
      "    ---\n",
      "     ### DSpy Workflow\n",
      "    \n",
      "    <img src=\"../assets/dspy_workflow.png\" >\n",
      "    \n",
      "    ---\n",
      "     ## Time to Put Words into Action ## Prepare Data\n",
      "    \n",
      "    We will scrape and extract text/markdown cells from all notebooks in this repository and prepare a dataset using the same. ## Setup Chroma\n",
      "    We started this workshop with **text representation** as one of the key components of any NLP system.\n",
      "    As we progressed from simple Bag of Words setup to highly contextualised Transformer models, we now have rich & dense representations.\n",
      "    The utility of such representations also increased multifold from word/sentence representations to features that can used for a number of downstream tasks.\n",
      "    \n",
      "    These representations, also called as vectors or embedding vectors are long series of numbers. Their retrieval and persistence requires specialised database management systems called **Vector Databases**.\n",
      "    \n",
      "    Vector Databases are particularly suited for handling data in the form of vectors, embeddings, or feature representations, which are commonly used in various applications like machine learning, natural language processing, computer vision, and recommendation systems.\n",
      "    \n",
      "    Key Features:\n",
      "    - High-dimensional Data Support\n",
      "    - Similarity Search\n",
      "    - Indexing Techniques\n",
      "    - Dimensionality Reduction\n",
      "    \n",
      "    There are a number of different off-the-shelf options available, such as:\n",
      "    - [ChromaDB](https://www.trychroma.com/)\n",
      "    - [PineCone](https://www.pinecone.io/)\n",
      "    - [Milvus](https://milvus.io/)\n",
      "    - [Weaviate](https://weaviate.io/)\n",
      "    - [AeroSpike](https://aerospike.com/)\n",
      "    - [OpenSearch](https://opensearch.org/)\n",
      "    \n",
      "    **Let's Install the Dependencies**\n",
      "    ```python\n",
      "    !pip install -q chromadb\n",
      "    !pip install retry\n",
      "    !pip install -U sentence-transformers\n",
      "    ```\n",
      "    \n",
      "    > ensure chroma is running on your terminal `$>chroma run --path ./chromadb` ## Vector Database: ChromaDB\n",
      "    \n",
      "    As mentioned above, there are a number of offering available. For this workshop we will make use of\n",
      "    [ChromaDB](https://www.trychroma.com/).\n",
      "    \n",
      "    It is a super simple setup which is easy to use. The following figure showcases the overall flow\n",
      "    \n",
      "    <img src=\"../assets/04_chroma_workflow.png\">\n",
      "    \n",
      "    > Source :[chromadb](https://docs.trychroma.com/)\n",
      "»»»\n",
      "[3] «««\n",
      "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop/blob/main/docs/module_04_llm_apps/01_retrieval_augmented_llm_app.ipynb\">\n",
      "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
      "    </a> # Retrieval Augmented LLM App\n",
      "    <img src=\"../assets/rap_banner.jpeg\">\n",
      "    \n",
      "    We have covered quite some ground in terms of understanding and building components for:\n",
      "    - Text Representation\n",
      "    - NLP Tasks\n",
      "    - Pretrained Models and Transfer Learning\n",
      "    - Model Fine-Tuning PEFT\n",
      "    - SFT and LLM Landscape\n",
      "    - Vector Databases\n",
      "    - Libraries and Frameworks\n",
      "    \n",
      "    Now we will work through development of an app to showcase how we can leverage all the concepts into a fully functioning system\n",
      "    \n",
      "    __Note__: In order to keep things simple, we will leverage most high-level APIs available but the overall setup should be easily extensible ## Why Retrieval Augmentation\n",
      "    \n",
      "    While theoretically LLMs are capable of having super long context windows, in real world settings this is a challenge because of:\n",
      "    - Inability/Limitation to ensure LLM focusses on correct sub-sections of the context\n",
      "    - High Memory requirements\n",
      "    - High API Cost\n",
      "    - High Latency , etc.\n",
      "    \n",
      "    \n",
      "    In order to overcome such challenges, we leverage vector databases to act as intelligent retrieval systems (again powered by LLMs) to:\n",
      "    - Provide focussed context\n",
      "    - Reduce memory, cost and latency requirements\n",
      "    - Unlock super-abilities to use upto-date information\n",
      "    - Offload trivial tasks to expert systems ## Streamlit Enters the Arena\n",
      "    \n",
      "    [Streamlit](https://streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. ## Let us Begin with Installation ### Start Chroma DB in Server mode\n",
      "    ```bash\n",
      "    >chroma run --path ./chromadb \n",
      "    ```\n",
      "    \n",
      "    ### Install NodeJS\n",
      "    ```bash\n",
      "    >curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n",
      "    \n",
      "    >export NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] && printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"\n",
      "    [ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\" # This loads nvm\n",
      "    \n",
      "    >source ~/.bashrc\n",
      "    >nvm install node\n",
      "    >npm install localtunnel\n",
      "    ``` > Following steps are useful for colab, on local systems start streamlit from terminal\n",
      "»»»\n",
      "[4] «««\n",
      "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop/blob/main/docs/module_04_llm_apps/03_mcp_getting_started.ipynb\">\n",
      "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
      "    </a> # Tool Calling & MCP \n",
      "    \n",
      "    <img src=\"../assets/04_tool_calling.png\">\n",
      "    \n",
      "    > Source: Generated with ❤️ Gemini 2.5 Flash ## Tool Calling\n",
      "    \n",
      "    Tool calling, or function calling, is a feature that allows LLMs to interact with external systems. Rather than just generating text, an LLM can be given a list of tools (like APIs or functions) it can use.\n",
      "    \n",
      "    ### How Does Tool Calling Work?\n",
      "    When a user provides a prompt, the LLM determines if a tool is needed. If so, it outputs a structured request (like a JSON object) detailing which tool to call and with what arguments. A separate program then executes this tool and returns the result to the LLM. The LLM uses this real-time information to formulate a more accurate and comprehensive final response, effectively extending its capabilities beyond its training data. ## Model Context Protocol (MCP) \n",
      "    \n",
      "    <img src=\"../assets/04_llm_mcp.png\">\n",
      "    \n",
      "    > Source: Generated with ❤️ Gemini 2.5 Flash\n",
      "    \n",
      "    ### Why MCP\n",
      "    LLMs are powerful but limited to their training data and can't interact with external systems, databases, or real-time information. MCP servers bridge this gap by providing secure, standardized connections between LLMs and external resources.\n",
      "    \n",
      "    ### What is MCP?\n",
      "    An **MCP (Model Context Protocol)** server is a lightweight service that exposes specific capabilities—like database queries, API calls, file operations, or tool integrations—to LLMs through a standardized protocol. It acts as a secure intermediary that allows AI models to access and interact with external systems while maintaining proper authentication and access controls.\n",
      "    \n",
      "    > Think of MCP like a USB-C port for AI applications\n",
      "    \n",
      "    \n",
      "    <img src=\"../assets/04_mcp.png\">\n",
      "    \n",
      "    > Source: [IBM](https://www.ibm.com/think/topics/model-context-protocol) ### MCP Server ### MCP Client > Go to terminal and from the module 4's directory execute : ``>python3 dummy_client.py``\n",
      "    \n",
      "    The following should be the output on your screen\n",
      "    <img src=\"../assets/04_mcp_client.png\">\n",
      "»»»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "List the models covered in module02\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "Module 02 focuses on training language models, specifically covering the GPT-2 model. It discusses the two-step training paradigm, including pretraining on large datasets and fine-tuning on task-specific data. The module also explains the components of transformer models, such as encoders, decoders, and attention mechanisms.\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "GPT-2\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b16b5-d6e4-44c1-889f-370f398ebd08",
   "metadata": {},
   "source": [
    "## How Does it All Go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbff9178-1c78-4a2b-968f-aa560d8ad29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4a340b4-39f4-477e-bf26-4b9b3951b0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question**:Which model is used for instruction fine-tuning?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: LLaMA 2-7B, 3.1-8B"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:List the models covered in module03"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: LLaMA, InstructGPT"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:Summarize key takeaways for module02"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: Training, tokenization, BERT, GPT"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:What is the focus of the following modules: module_01,module_02,module_03 and module_04? Respond as a list"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: - Module_01: Text representation and tokenization  \n",
       "- Module_02: Training language models  \n",
       "- Module_03: Instruction tuning and alignment  \n",
       "- Module_04: Advanced applications (tool calling, MCP, vector databases)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:For RLHF what policy is covered in module03?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: Proximal Policy Optimization"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Which model is used for instruction fine-tuning?\",\n",
    "    \"List the models covered in module03\",\n",
    "    \"Summarize key takeaways for module02\",\n",
    "    \"What is the focus of the following modules: module_01,module_02,module_03 and module_04? Respond as a list\",\n",
    "    \"For RLHF what policy is covered in module03?\"\n",
    "]\n",
    "\n",
    "uncompiled_baleen = SimplifiedBaleen(passages_per_hop=5)  # uncompiled (i.e., zero-shot) program\n",
    "for question in questions:\n",
    "    display(Markdown(f\"**Question**:{question}\"))\n",
    "    pred = uncompiled_baleen(question)\n",
    "    display(Markdown(f\"**Predicted Answer**: {pred.answer}\"))\n",
    "    display(Markdown(\"**Retrieved Contexts (truncated)**:\"))\n",
    "    for c in pred.context:\n",
    "        print(f\"{c[:100]}...\" )\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696c37a-0f5a-435f-8e67-e1e9885c1a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
