# Instruction Tuning and Alignment

This module covers instruction tuning and alignment techniques for LLMs, including supervised fine-tuning, RLHF, and DPO. Learn how to make LLMs follow instructions and align with human preferences.

## Table of Contents
- [Instruction Tuning with Llama: Text-to-Python](01_instruction_tuning_llama_txt2py.ipynb)
- [RLHF with Phi-2](02_RLHF_phi2.ipynb)
- [DPO and Zephyr Alignment](03_zephyr_alignment_dpo.ipynb)

---

### Instruction Tuning with Llama: Text-to-Python
[01_instruction_tuning_llama_txt2py.ipynb](01_instruction_tuning_llama_txt2py.ipynb)
- Learn how to fine-tune LLMs to follow instructions using supervised datasets, with practical examples for code generation.

### RLHF with Phi-2
[02_RLHF_phi2.ipynb](02_RLHF_phi2.ipynb)
- Explore Reinforcement Learning from Human Feedback (RLHF) using the Phi-2 model, including reward modeling and preference optimization.

### DPO and Zephyr Alignment
[03_zephyr_alignment_dpo.ipynb](03_zephyr_alignment_dpo.ipynb)
- Understand Direct Preference Optimization (DPO) and advanced alignment strategies for safer, more helpful LLMs.

> _README auto-generated by GitHub Copilot on 2025-08-10_
---
