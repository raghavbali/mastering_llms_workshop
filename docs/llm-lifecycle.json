{
  "header": {
    "title": "Large Language Model Training Lifecycle",
    "subtitle": "Complete overview of LLM training methods and approaches",
    "themeToggle": {
      "text": "ğŸŒ“ Toggle Theme",
      "id": "themeToggle"
    }
  },
  "summary": {
    "title": "ğŸ“Š Training Pipeline Overview",
    "stages": [
      {
        "id": "pretraining",
        "icon": "ğŸ¤–",
        "text": "Pre-training"
      },
      {
        "id": "branching",
        "icon": "ğŸ”€",
        "text": "Post-Training Paths",
        "special": "branching-box"
      },
      {
        "id": "branches",
        "type": "branch-group",
        "stages": [
          {
            "id": "branching",
            "icon": "ğŸ¯",
            "text": "Task-Specific Fine-tuning"
          },
          {
            "id": "branching",
            "icon": "ğŸ“",
            "text": "Instruction Tuning"
          }
        ]
      },
      {
        "id": "branching",
        "icon": "âš–ï¸",
        "text": "Alignment Methods"
      },
      {
        "id": "applications",
        "icon": "ğŸš€",
        "text": "Applications"
      }
    ]
  },
  "phases": {
    "title": "<br/>ğŸ” Detailed Training Phases",
    "phases": [
      {
        "id": "pretraining",
        "title": "ğŸ¤– Phase 1: Pre-training",
        "badge": {
          "text": "Foundation",
          "class": "foundation"
        },
        "collapsed": true,
        "info": [
          {
            "label": "ğŸ“¥ Input:",
            "value": "Raw internet text (10T+ tokens)"
          },
          {
            "label": "ğŸ“¤ Output:",
            "value": "Base language model with general understanding"
          },
          {
            "label": "â±ï¸ Time:",
            "value": "3 months (90-100 days)"
          },
          {
            "label": "ğŸ–¥ï¸ Compute:",
            "value": "25,000 NVIDIA A100 GPUs"
          },
          {
            "label": "ğŸ¯ Objective:",
            "value": "Learn fundamental language patterns through next-token prediction"
          },
          {
            "label": "ğŸ‘¨ğŸ»â€ğŸ’» Hands-on:",
            "value": "<ul><li><a href=\"module_01_lm_fundamentals/01_text_representation.ipynb\">Text Representation</a></li><li><a href=\"module_01_lm_fundamentals/02_contextual_embeddings.ipynb\">Contextual Embeddings</a></li><li><a href=\"module_02_llm_building_blocks/01_transformers.ipynb\">Transformers</a></li><li><a href=\"module_02_llm_building_blocks/03_training_language_models.ipynb\">Training Language Models</a></li></ul>"
          }
        ]
      },
      {
        "id": "branching",
        "title": "ğŸ”€ Phase 2: Post Training",
        "badge": {
          "text": "Decision Point",
          "class": "branching"
        },
        "collapsed": true,
        "description": "After pre-training, models can follow two main paths based on the intended use case:",
        "branches": [
          {
             "id": "task-specific",
            "title": "ğŸ¯ Path A: Task-Specific Fine-tuning",
            "class": "task-specific",
            "info": [
              {
                "label": "ğŸ“¥ Input:",
                "value": "Task-specific labeled data (1K-100K examples)"
              },
              {
                "label": "ğŸ“¤ Output:",
                "value": "Specialized Models/Task-Tuned Models for classification, Q&A, summarization, etc."
              },
              {
                "label": "â±ï¸ Time:",
                "value": "1-2 weeks"
              },
              {
                "label": "ğŸ–¥ï¸ Compute:",
                "value": "100+ GPUs"
              },
              {
                "label": "ğŸ¯ Objective:",
                "value": "Adapt for specific downstream tasks"
              },
              {
                "label": "ğŸ“‹ Examples:",
                "value": "Text classification, sentiment analysis, question answering, translation, summarization"
              },
              {
                "label": "ğŸ‘¨ğŸ»â€ğŸ’» Hands-on:",
                "value": "<ul><li><a href=\"module_02_llm_building_blocks/02_transformers_pipelines.ipynb\">Transformer Pipelines</a></li></ul>"
              }
            ]
          },
          {
            "title": "ğŸ“ Path B: Instruction Tuning Path",
            "steps": [
              {
                "title": "ğŸ“š Step 1: Supervised Fine-tuning (SFT)",
                "class": "instruction-sft",
                "info": [
                  {
                    "label": "ğŸ“¥ Input:",
                    "value": "Instruction-response pairs (10K-100K examples)"
                  },
                  {
                    "label": "ğŸ“¤ Output:",
                    "value": "Instruction-following model"
                  },
                  {
                    "label": "â±ï¸ Time:",
                    "value": "1-2 weeks"
                  },
                  {
                    "label": "ğŸ–¥ï¸ Compute:",
                    "value": "1000-10,000 GPUs"
                  },
                  {
                    "label": "ğŸ¯ Objective:",
                    "value": "Teach model to follow general instructions"
                  },
                  {
                    "label": "ğŸ‘¨ğŸ»â€ğŸ’» Hands-on:",
                    "value": "<ul><li><a href=\"module_03_instruction_tuning_and_alignment/01_instruction_tuning_llama_txt2py.ipynb\">Instruction Tuning LLaMA</a></li></ul>"
                  }
                ]
              },
              {
                "title": "âš–ï¸ Step 2: Alignment Methods",
                "hasMethodTabs": true,
                "methods": [
                  {
                    "id": "rlhf",
                    "name": "RLHF",
                    "fullName": "RLHF (Reinforcement Learning from Human Feedback)",
                    "year": "2022",
                    "complexity": "high",
                    "complexityText": "High Complexity",
                    "class": "rlhf",
                    "active": true,
                    "info": [
                      {
                        "label": "ğŸ“¥ Input:",
                        "value": "Human preference rankings (1K-10K pairs)"
                      },
                      {
                        "label": "ğŸ”„ Process:",
                        "value": "3-stage (SFT â†’ Reward Model â†’ PPO)"
                      },
                      {
                        "label": "âœ… Pros:",
                        "value": "Human-aligned responses, proven effectiveness"
                      },
                      {
                        "label": "âŒ Cons:",
                        "value": "Complex, expensive, requires human feedback"
                      },
                      {
                        "label": "ğŸ‘¨ğŸ»â€ğŸ’» Hands-on:",
                        "value": "<ul><li><a href=\"module_03_instruction_tuning_and_alignment/02_RLHF_phi2.ipynb\">RLHF</a></li></ul>"
                      }
                    ]
                  },
                  {
                    "id": "dpo",
                    "name": "DPO",
                    "fullName": "DPO (Direct Preference Optimization)",
                    "year": "2023",
                    "complexity": "medium",
                    "complexityText": "Medium Complexity",
                    "class": "dpo",
                    "info": [
                      {
                        "label": "ğŸ“¥ Input:",
                        "value": "Preference pairs (chosen/rejected)"
                      },
                      {
                        "label": "ğŸ”„ Process:",
                        "value": "2-stage (SFT â†’ Direct optimization)"
                      },
                      {
                        "label": "âœ… Pros:",
                        "value": "Simpler than RLHF, no reward model needed"
                      },
                      {
                        "label": "âŒ Cons:",
                        "value": "Still requires preference data"
                      },
                      {
                        "label": "ğŸ‘¨ğŸ»â€ğŸ’» Hands-on:",
                        "value": "<ul><li><a href=\"module_03_instruction_tuning_and_alignment/01_instruction_tuning_llama_txt2py.ipynb\">DPO Gemma</a></li></ul>"
                      }
                    ]
                  },
                  {
                    "id": "orpo",
                    "name": "ORPO",
                    "fullName": "ORPO (Odds Ratio Preference Optimization)",
                    "year": "2024",
                    "yearClass": "new",
                    "complexity": "low",
                    "complexityText": "Low Complexity",
                    "class": "orpo",
                    "info": [
                      {
                        "label": "ğŸ“¥ Input:",
                        "value": "Preference pairs"
                      },
                      {
                        "label": "ğŸ”„ Process:",
                        "value": "1-stage (combines SFT and alignment)"
                      },
                      {
                        "label": "âœ… Pros:",
                        "value": "Single-step process, no reference model needed"
                      },
                      {
                        "label": "âŒ Cons:",
                        "value": "Newer method, less tested"
                      }
                    ]
                  },
                  {
                    "id": "rlaif",
                    "name": "RLAIF",
                    "fullName": "RLAIF (Reinforcement Learning from AI Feedback)",
                    "year": "2023",
                    "complexity": "medium",
                    "complexityText": "Medium Complexity",
                    "class": "rlaif",
                    "info": [
                      {
                        "label": "ğŸ“¥ Input:",
                        "value": "AI-generated feedback"
                      },
                      {
                        "label": "ğŸ”„ Process:",
                        "value": "Similar to RLHF but with AI feedback"
                      },
                      {
                        "label": "âœ… Pros:",
                        "value": "Scalable, no human feedback needed"
                      },
                      {
                        "label": "âŒ Cons:",
                        "value": "Dependent on AI evaluator quality"
                      }
                    ]
                  },
                  {
                    "id": "constitutional",
                    "name": "Constitutional",
                    "fullName": "Constitutional AI",
                    "year": "2022",
                    "complexity": "medium",
                    "complexityText": "Medium Complexity",
                    "class": "constitutional",
                    "info": [
                      {
                        "label": "ğŸ“¥ Input:",
                        "value": "Self-critique with constitution rules"
                      },
                      {
                        "label": "ğŸ”„ Process:",
                        "value": "2-stage (Self-improvement â†’ RL with AI feedback)"
                      },
                      {
                        "label": "âœ… Pros:",
                        "value": "Transparent principles, self-supervised"
                      },
                      {
                        "label": "âŒ Cons:",
                        "value": "Rule design required"
                      }
                    ]
                  },
                  {
                    "id": "grpo",
                    "name": "GRPO",
                    "fullName": "Group Relative Policy Optimization",
                    "year": "2025",
                    "complexity": "high",
                    "complexityText": "High Complexity",
                    "class": "deepseek",
                    "info": [
                      {
                        "label": "ğŸ“¥ Input:",
                        "value": "Seeded with 1k CoT examples(``<think>``) followed by self-generated responses"
                      },
                      {
                        "label": "ğŸ”„ Process:",
                        "value": "Multi-stage (Coldstart CoT â†’ GRPO with Rule based verfication â†’ Post Alignment)"
                      },
                      {
                        "label": "âœ… Pros:",
                        "value": "Improved reasoning capabilities, better at math and coding"
                      },
                      {
                        "label": "âŒ Cons:",
                        "value": "CoT dataset preparation and reward verification quality"
                      }
                    ]
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "id": "applications",
        "title": "ğŸš€ Phase 3: LLM Based Applications",
        "badge": {
          "text": "Live System",
          "class": "production"
        },
        "collapsed": true,
        "info": [
          {
            "label": "ğŸ“¥ Input:",
            "value": "Aligned model from chosen method â†’ Safety Guardrails & Red Teaming â†’ Quantization & Distillation as needed"
          },
          {
            "label": "ğŸ“¤ Output:",
            "value": "Production-ready LLM"
          },
          {
            "label": "ğŸ”„ Ongoing:",
            "value": "Evaluation, monitoring, updates"
          },
          {
            "label": "ğŸ‘¨ğŸ»â€ğŸ’» Hands-on:",
            "value": "<ul><li><a href=\"module_04_llm_ops/04_retrieval_augmented_llm_app.ipynb\">RAG</a></li><li><a href=\"module_04_llm_ops/05_dspy_demo.ipynb\">DSPy</a></li></ul>"
          }
        ]
      }
    ]
  },
  "timeline": {
    "title": "ğŸ“… Evolution Timeline",
    "items": [
      {
        "year": "2018",
        "content": "<b>Task-Specific Fine-tuning</b>: <i>BERT, GPT</i>"
      },
      {
        "year": "2019",
        "content": "<b>Large-Scale Pre-training</b>: <i>GPT-2, T5</i>"
      },
      {
        "year": "2021",
        "content": "<b>Instruction Tuning (SFT)</b>: <i> FLAN, Instruct-GPT</i>"
      },
      {
        "year": "2022",
        "content": "<b>RLHF</b>: <i>GPT-3.5/ChatGPT, LLaMA</i>"
      },
      {
        "year": "2023",
        "content": "<b>DPO</b>: <i> LLaMA2, MS-Phi, GPT4, Gemini</i>"
      },
      {
        "year": "2024",
        "content": "<b>ORPO</b>: <i> Claude 3.5, LLaMA-3, GPT-4o</i>"
      },
      {
        "year": "2025",
        "content": "<b>GRPO</b>: <i>DeepSeek, Claude-4/Opus</i>"
      }
    ]
  }
}