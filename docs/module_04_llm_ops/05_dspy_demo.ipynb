{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ed0667c-fe9b-45b4-83b3-2ad4ead66225",
   "metadata": {},
   "source": [
    "# <img src=\"./assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
    "---\n",
    "<img src=\"./assets/dspy_banner.png\">\n",
    "\n",
    "---\n",
    "\n",
    "## Intro\n",
    "- Language Models are like extremely complex machines with capabilities to retrieve and reformulate information from an **extremely large latent space**.\n",
    "- To guide this search and achieve desired responses we heavily rely on **complex, long and brittle prompts** which (at times) are very specific to certain LLMs\n",
    "- Being an open area of research, teams are working from different perspectives to abstract and enable rapid development of **LLM-enabled systems**.\n",
    "- **StanfordDSpy** is one such framework for algorithmally optimizing LM prompts and weights.\n",
    "\n",
    "\n",
    "> $_{DSpy\\ logo\\ is\\ copyright/ownership\\ of\\ respective\\ teams}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39a983-e508-43a3-aaed-7c388a0be75b",
   "metadata": {},
   "source": [
    "## Ok, You Got Me Intrigued, Tell Me More?\n",
    "\n",
    "- The DSpy framework takes inspiration from deep learning frameworks such as <img src=\"./assets/pytorch_logo.png\" width=\"2%\">[PyTorch](https://pytorch.org/)\n",
    "    - For instance, to build a deep neural network using PyTorch we simply use standard layers such as ``convolution``, ``dropout``, ``linear`` and attach them to optimizers like ``Adam`` and train without worrying about implementing these from scratch everytime.\n",
    "- Similarly, DSpy provides a a set of standard general purpose **modules** (``ChainOfThought``,``Predict``), **optimizers** (``BootstrapFewShotWithRandomSearch``) and helps us build systems by composing these components as layers into a ``Program`` without explicitly dealing with prompts! Neat isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf9d1b7-8030-433a-a830-3f506cabc010",
   "metadata": {},
   "source": [
    "### Usual Prompt Based Workflow\n",
    "<img src=\"./assets/prompt_workflow.png\" width=\"75%\">\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e534227-f2b7-4acb-a7e9-91c47e33f769",
   "metadata": {},
   "source": [
    "### LangChain-Like Workflow\n",
    "\n",
    "<img src=\"./assets/langchain_workflow.png\" width=\"75%\">\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f045e-8319-433f-bbe1-ff858bcd4d73",
   "metadata": {},
   "source": [
    "### DSpy Workflow\n",
    "\n",
    "<img src=\"./assets/dspy_workflow.png\" >\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2a7ff-28f1-4cdc-a35a-0b63e7d4cb1b",
   "metadata": {},
   "source": [
    "## Time to Put Words into Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3501ddfa-233e-4b74-95a8-9da46ea63e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import dspy\n",
    "from dsp.utils import deduplicate\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from scraper_utils import NB_Markdown_Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e48faf88-dabe-4c97-95d8-7ab6cd467cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be modular \n",
    "OPENAI_TOKEN = '<YOUR TOKEN>'\n",
    "llm_model = dspy.OpenAI(model='gpt-4o-mini',\n",
    "                    api_key=OPENAI_TOKEN,\n",
    "                    max_tokens=1024*4,\n",
    "                    temperature=0.7, \n",
    "                    model_type=\"chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a20e56f-4061-4217-8db6-631d55ade272",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "We will scrape and extract text/markdown cells from all notebooks in this repository and prepare a dataset using the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59bd1bea-e566-4cac-8238-42713c59dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_scraper = NB_Markdown_Scraper([f'../module_0{i}' for i in range(1,5)])\n",
    "nb_scraper.scrape_markdowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c8b266-e915-46aa-bfd0-a0af44f5d64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['module_01_03_explore_transformers', 'module_01_02_getting_started', 'module_02_02_simple_text_generator', 'module_03_02_instruction_tuning_llama_t2sql', 'module_03_01_llm_training_and_scaling', 'module_03_03_RLHF_phi2', 'module_04_04_retrieval_augmented_llm_app', 'module_04_02_vector_databases_hf_inference_endpoint', 'module_04_03_OpenSource_ClosedSource_LLMs', 'module_04_01_prompt_engineeering_and_langchain', 'module_04_05_dspy_demo'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_scraper.notebook_md_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b5e711-ac90-4ece-bdd8-6905d9f2a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dspy_content.tsv\", \"w\") as record_file:\n",
    "    for k,v in nb_scraper.notebook_md_dict.items():\n",
    "        record_file.write(f\"{k}\\t{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53358b2d-7930-449e-9008-95a8f180b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = []\n",
    "ctr = 1\n",
    "for k,_ in nb_scraper.notebook_md_dict.items():\n",
    "    doc_ids.append(f'{ctr}_{k}')\n",
    "    ctr+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef35ce5-13b9-494a-82e0-16195bd12099",
   "metadata": {},
   "source": [
    "## Setup Chroma\n",
    "> ensure chroma is running on your terminal `$>chroma run --path ./chromadb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a1d418-aa47-4ff2-bc75-ba1f8318095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "chroma_emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "client = chromadb.HttpClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70cddc6-f946-454b-b7a4-6ff5c64b82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_COLLECTION_NAME = \"workshop_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a26fb8bb-0fe8-4701-84f8-d7a23fcf71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(CHROMA_COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "716310d4-9f55-41f3-a6b9-f509c7680c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\n",
    "    CHROMA_COLLECTION_NAME,\n",
    "    embedding_function=chroma_emb_fn,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e95f64-c238-4a86-94b2-d3f040346d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to collection\n",
    "collection.add(\n",
    "    documents=[v for _,v in nb_scraper.notebook_md_dict.items()], \n",
    "    ids=doc_ids, # must be unique for each doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2443ccd3-7d06-4071-9657-408cba6d4fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6_module_03_03_RLHF_phi2', '2_module_01_02_getting_started', '3_module_02_02_simple_text_generator']\n",
      "[0.6174977412306334, 0.8062083377747705, 0.8820602339897555]\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"RLHF\"], # Chroma will embed using the function we provided\n",
    "    n_results=3 # how many results to return\n",
    ")\n",
    "print(results['ids'][0])\n",
    "print(results['distances'][0])\n",
    "#print([i[:100] for j in results['documents'] for i in j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c036bf-6144-4945-9732-0ddebd2426f4",
   "metadata": {},
   "source": [
    "# Chroma as RM for DSPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15544929-dbd3-4598-9c60-a193be2c7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5fcb6f8-f674-44c1-ab1d-3d8c489c2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "import os\n",
    "import openai\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "475245a5-37ea-4e60-8bda-60c10f0c490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id::6_module_03_03_RLHF_phi2\n",
      "Document score::0.6174977412306334\n",
      "Document: # Quick Overview of RLFH\n",
      "\n",
      "The performance of Langu ... \n",
      "\n",
      "Document id::2_module_01_02_getting_started\n",
      "Document score::0.8062083377747705\n",
      "Document: # Getting Started : Text Representation\n",
      "<img src=\" ... \n",
      "\n",
      "Document id::3_module_02_02_simple_text_generator\n",
      "Document score::0.8820602339897555\n",
      "Document: # Text Generation <a target=\"_blank\" href=\"https:/ ... \n",
      "\n",
      "Document id::11_module_04_05_dspy_demo\n",
      "Document score::0.9200280698248913\n",
      "Document: # <img src=\"./assets/dspy_logo.png\" width=\"2%\"> DS ... \n",
      "\n",
      "Document id::8_module_04_02_vector_databases_hf_inference_endpoint\n",
      "Document score::0.947110437471832\n",
      "Document: ## Vector Databases\n",
      "\n",
      "<img src=\"./assets/vector_ban ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever_model = ChromadbRM(\n",
    "    CHROMA_COLLECTION_NAME,\n",
    "    './chromadb/',\n",
    "    embedding_function=chroma_emb_fn,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "results = retriever_model(\"RLHF\", k=5)\n",
    "\n",
    "for result in results:\n",
    "    print(f'Document id::{result.id}')\n",
    "    print(f'Document score::{result.score}')\n",
    "    print(\"Document:\", result.long_text[:50],'...' ,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb257f26-83be-492a-bd2f-0f2fb6803895",
   "metadata": {},
   "source": [
    "## Basic DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e669fa46-374c-400c-bd62-d29165822918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LM and RM\n",
    "dspy.settings.configure(lm=llm_model,rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c738473d-a729-4e7c-99a7-1736bd3ecffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c4a40f-a3c6-4cf5-8129-be079ba4e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deb768da-94f8-4c47-81ce-191e476daab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List the models covered in module03\n",
      "Predicted Answer: GPT, BERT, T5\n",
      "Retrieved Contexts (truncated):# Prompt Engineering\n",
      "<img src=\"./assets/pe_banner.jpg\">\n",
      "\n",
      "Prompt Engineering is this thrilling new di...\n",
      "Retrieved Contexts (truncated):# Scaling Neural Nets and Efficient Training\n",
      "\n",
      "We have covered quite some ground in previous 2 module...\n",
      "Retrieved Contexts (truncated):# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_w...\n"
     ]
    }
   ],
   "source": [
    "my_question = \"List the models covered in module03\"\n",
    "compiled_rag = RAG()\n",
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "pred = compiled_rag(my_question)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "for c in pred.context:\n",
    "    print(f\"Retrieved Contexts (truncated):{c[:100]}...\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529bb6a-e39f-4a07-b0f0-33c53f478f61",
   "metadata": {},
   "source": [
    "## Multi-Hop DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f7f79f4-b908-4e76-833b-4da612550ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1261053-1212-476d-81f2-6eb4329fee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateSearchQuery(dspy.Signature):\n",
    "    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    query = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b650da3-a6fa-4a9b-869b-8d07ae96013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBaleen(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_query[hop](context=context, question=question).query\n",
    "            passages = self.retrieve(query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        pred = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=pred.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f13c28c6-1e04-47dd-a6f5-25c6581c134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List the models covered in module03\n",
      "Predicted Answer: GPT-2, BERT, T5\n",
      "Retrieved Contexts (truncated):# Prompt Engineering\n",
      "<img src=\"./assets/pe_banner.jpg\">\n",
      "\n",
      "Prompt Engineering is this thrilling new di...\n",
      "Retrieved Contexts (truncated):# Open Source Vs Close Sourced LLMs\n",
      "\n",
      "Similar to any other piece of technology, LLMs are available in...\n",
      "Retrieved Contexts (truncated):# <img src=\"./assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
      "---\n",
      "<img src=\"./assets/dspy_b...\n",
      "Retrieved Contexts (truncated):# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_w...\n"
     ]
    }
   ],
   "source": [
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "uncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program\n",
    "pred = uncompiled_baleen(my_question)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "for c in pred.context:\n",
    "    print(f\"Retrieved Contexts (truncated):{c[:100]}...\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead70e81-07f3-4187-8e22-f7100c6e14ae",
   "metadata": {},
   "source": [
    "## Let Us Add Some Checks/Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7c79924-ea95-4d0b-b915-cc6d585b15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckContextuality(dspy.Signature):\n",
    "    \"\"\"Check if the generated response is from the provided context\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    response = dspy.InputField(desc=\"generated response to question\")\n",
    "    is_contextual = dspy.OutputField(desc=\"generate a boolean response as True if the response is based on context otherwise respond with False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11a5a37e-1a96-4e80-bdc7-8294ac3eb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBaleenwithAssertions(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.checkcontextuality = dspy.Predict(CheckContextuality)\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_query[hop](context=context, question=question).query\n",
    "            passages = self.retrieve(query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        pred = self.generate_answer(context=context, question=question)\n",
    "        response = dspy.Prediction(context=context, answer=pred.answer)\n",
    "        dspy.Suggest(\n",
    "            self.checkcontextuality(context=context,response=response.answer).is_contextual.lower()=='true',\n",
    "            \"Response should be factual and based on provided context only\",\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f6a24e6-e92c-4393-8136-e89e485226cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from dspy.primitives.assertions import assert_transform_module, backtrack_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16419bcf-948d-4caa-bb6e-648e6180f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtrack_handler\n",
    "dspy_program_with_assertions_retry_once = assert_transform_module(SimplifiedBaleenwithAssertions(), \n",
    "    functools.partial(backtrack_handler, max_backtracks=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f29f81a1-77a2-43ad-90cc-a8591e3968e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the Prime Minister of India?\n",
      "Predicted Answer: Narendra Modi\n",
      "Retrieved Contexts (truncated):# Scaling Neural Nets and Efficient Training\n",
      "\n",
      "We have covered quite some ground in previous 2 module...\n",
      "Retrieved Contexts (truncated):# Prompt Engineering\n",
      "<img src=\"./assets/pe_banner.jpg\">\n",
      "\n",
      "Prompt Engineering is this thrilling new di...\n",
      "Retrieved Contexts (truncated):# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_w...\n"
     ]
    }
   ],
   "source": [
    "out_of_context_question = \"Who is the Prime Minister of India?\"\n",
    "pred = dspy_program_with_assertions_retry_once(out_of_context_question)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {out_of_context_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "for c in pred.context:\n",
    "    print(f\"Retrieved Contexts (truncated):{c[:100]}...\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc408a11-7281-4e62-9eea-c230c1b8e4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Check if the generated response is from the provided context\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Response: generated response to question\n",
      "\n",
      "Previous Is Contextual: past Is Contextual: with errors\n",
      "\n",
      "Instructions: Some instructions you must satisfy\n",
      "\n",
      "Is Contextual: generate a boolean response as True if the response is based on context otherwise respond with False\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «# Scaling Neural Nets and Efficient Training\n",
      "\n",
      "We have covered quite some ground in previous 2 modules and observed the steady increase in size and performance of the models. These gains come at huge cost, actual money and human labour apart from time researching and building these things. Can we estimate these costs and draw some insights about model sizes, datasets and comput requirements? ## Estimating Compute Costs\n",
      "> Back of the Envelope Calculations : A quick way to get rough estimates\n",
      "\n",
      "\n",
      "**[LLaMA 3.1](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)** from Meta.AI launched very recently. The model is available in 8B, 70B and 405B sizes and is outperforming a number of existing LLMs on various benchmarks. \n",
      "\n",
      "![image.png](attachment:08264d12-83d1-45ac-8664-90c3f5af5ad6.png) ## So Should We Just Keep Growing?\n",
      "\n",
      "**TL;DR**: Probably not! \n",
      "\n",
      "**Long Answer**: In their work titled [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556) Hoffman et. al. build upon the previous works to showcase that current(_2022_) set of models are **significantly under trained** or the current set of LLMs are far too large for their compute budgets and datasets!\n",
      "\n",
      "They present a 70B parameter model titled **Chincilla** which was:\n",
      "- 4x smaller than 280B parameter Gopher\n",
      "- trained on 4x more data than Gopher, 1.3T tokens vs 300B tokens\n",
      "\n",
      "and yet **outperformed** Gopher on every task they evaluated!\n",
      "\n",
      "<img src=\"./assets/chinchilla.png\">\n",
      "\n",
      "> Source: [Hoffman et. al.](https://arxiv.org/pdf/2203.15556)\n",
      "> Fine-print: Though undertrained, LLMs increasingly show performance improvement with increasing dataset size ## Ok, So I have a lot of Compute, What's the Problem?\n",
      "\n",
      "The scaling laws are all good for BigTech, but you could say that most companies have a lot of compute available. Where is the problem? Let us understand this with a simple example walk through\n",
      "\n",
      "Assumptions/Setup:\n",
      "- System RAM (CPU): 32GB\n",
      "- GPU RAM : 32 GB\n",
      "- Model Size : 20B\n",
      "- Parameter Size: 2bytes \n",
      "This is good for inference but we need to train/fine-tune this model.\n",
      "We need to accomodate for:\n",
      "- **Gradients/backpropagation** : Size same as model size\n",
      "- **Optimizer States** (ex: ADAM needs momentum and variance, can't be FP16): typically 12x of model size We need more memory (and faster GPUs). But just by usual scaling we would need: ## Can We Bring Some Efficiencies? Please? Yes, luckily researchers have been exploring efficient ways of training and fine-tuning models to democratise availability of such models across a broader spectrum (and lower the environmental impact as well, yay!!!)\n",
      "\n",
      "We will focus on three key main efficiencies under the umbrella of **P**arameter **E**fficient **F**ine **T**uning paradigm:\n",
      "+ Quantization\n",
      "+ Additive PEFT\n",
      "    + Soft Prompting\n",
      "+ Reparameterization\n",
      "    + LoRA\n",
      " \n",
      "There are other techniques as well, such as, LoHa, LoKr, IA3, P-Tuning and more. While effective, the basic ideas are similar to what we will cover next. ### Quantization #### Basic Idea\n",
      "- The basic idea is to limit the number of bits/bytes needed to store our weights/parameters/matrix-cell values.\n",
      "- By default we store weights as high-precision floats in float32 or even float64 occupying enormous amounts of space (disk/memory).\n",
      "- Can we be smart and store those values without consuming all 32 or 64 bits?\n",
      "\n",
      "#### Let us understand this with a very simplified example:\n",
      "\n",
      "<img src=\"./assets/quantization.png\"> ### Additive PEFT Additive PEFT works by adding new tunable layers to the model and train only those during fine-tuning while keeping the base model weights frozen. \n",
      "\n",
      "But how do we do this?\n",
      "\n",
      "#### Soft Prompting\n",
      "- This technique involves introducing **task specific tokens** or **virtual tokens** to the model's input space\n",
      "- The virtual tokens are not part of the actual vocabulary of the model and only specify the task.\n",
      "- The dimensionality of virtual tokens is same as the input token embedding size\n",
      "- During finetuning the base model weights are frozen and only virtual token embedding layer is trained/updated.\n",
      "- Supports Mix-task Batch Fine-Tuning and hence no need for separate heads\n",
      "- Setup is similar to prefix tuning technique\n",
      "\n",
      "#### Overview of Soft Prompting\n",
      "\n",
      "<img src=\"./assets/soft_prompting_1.png\">\n",
      "\n",
      "---\n",
      "\n",
      "<img src=\"./assets/soft_prompting_2.png\">\n",
      "\n",
      "---\n",
      "\n",
      "This technique is shown to be better/efficient and more stable than **manual prompting**.\n",
      "<img src=\"./assets/soft_prompting_perf.png\">\n",
      "\n",
      "> Source: https://arxiv.org/pdf/2104.08691\n",
      " ## But how much does it cost to train such model(s)?\n",
      "<img src=\"./assets/cost_tweet.png\">\n",
      "\n",
      "> Source: https://x.com/deedydas/status/1629312480165109760\n",
      "\n",
      "__Assumptions__\n",
      "For the sake of our understanding, we will make the following assumptions:\n",
      "- Ignore costs associated with preparing datasets\n",
      "- Ignore costs associated with training restarts, infra-failures, etc.\n",
      "- Cost of forward and backward pass is set to 1\n",
      "- Assume a very simplified view of overhead associated with multi-GPU/multi-node clusters by setting a standard efficiency ratio (ex: 0.25 efficiency in terms of TFLOPs) ### Reparametrization\n",
      "This technique smartly leverages matrix decomposition to bring efficiencies. Basically during fine-tuning:\n",
      "- we freeze the base model weights and\n",
      "- during the backward pass, we decompose the **weight update matrix** ($W_\\delta$) into two lower rank matrices $W_a$ and $W_b$ of rank $r$\n",
      "- This helps in achieving 100 to 1000x reduction in weights to be updated.\n",
      "\n",
      "The following illustration explains the same:\n",
      "<img src=\"./assets/lora_1.png\"> > qLoRA combines the improvements of quantization to LoRA thus leading to even further improvements ### Model Parameters\n",
      "- Model Size : 405 **B**illion\n",
      "- Training Dataset : 15 **T**rillion ### Compute Required  ### GPU Performance and Compute Time ### Cost of Training ## Big but How Big?\n",
      "\n",
      "The latest and the greatest seem to be a thing only the _GPU-rich_ can afford to play with. The exponential increase in the size of models along with their training datasets (we saw GPT vs GPT2 vs GPT3.5 in the previous module) indicates scale is our best friend. \n",
      "\n",
      "Work by Kaplan et. al. in the work titled **[Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361)** presents some interesting takeaways. \n",
      "We will use the notation from paper as:\n",
      "- **$N$**: Model parameters excluding embeddings\n",
      "- **$D$**: Size of the dataset\n",
      "- **$C$**: Compute used for training the model\n",
      "\n",
      "_Scale is a function of $N$, $D$ and $C$_\n",
      "\n",
      "\n",
      "Let's look at some of the insights from the paper: 1. Performance depends **strongly on scale** and weakly on model shape\n",
      "2. Performance improves predictably as long as we **scale up** **$N$** and **$D$** : \n",
      "_Every time we increase model size 8x, we only need to increase the dataset by roughly 5x_\n",
      "3. Large Models are more **sample efficient** than small models reaching same level of performance with fewer steps and fewer data points <img src=\"./assets/scaling_laws.png\">\n",
      "\n",
      "> Source: [Kaplan et. al.](https://arxiv.org/pdf/2001.08361)»\n",
      "[2] «# Prompt Engineering\n",
      "<img src=\"./assets/pe_banner.jpg\">\n",
      "\n",
      "Prompt Engineering is this thrilling new discipline that opens the door to a world of possibilities with large language models (LLMs).\n",
      "\n",
      "As a prompt engineer, you'll delve into the depths of LLMs, unraveling their capabilities and limitations with finesse. But prompt engineering isn't about mere prompts. It is aa combination of skills and techniques, enabling you to interact and innovate through the use of LLMs.\n",
      "\n",
      "In this module, we will step into the fascinating world of prompt engineering, where we will learn about key principals of working with LLMs through prompts.\n",
      "\n",
      "## Local Model using GPT4ALL\n",
      "> GPT4All is an open-source software ecosystem that allows anyone to train and deploy powerful and customized large language models (LLMs) on everyday hardware. Nomic AI oversees contributions to the open-source ecosystem ensuring quality, security and maintainability.\n",
      "\n",
      "It provides easy to setup and use python bindings.\n",
      "\n",
      "```python\n",
      "!pip install gpt4all\n",
      "```\n",
      "\n",
      "For OpenAI bindings\n",
      "```python\n",
      "!pip install --upgrade openai\n",
      "``` <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop_dhs23/blob/main/module_04/prompt_engineeering_and_langchain.ipynb\">\n",
      "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
      "</a> ## Prompting Basics\n",
      "\n",
      "+ Be Clear and Provide Specific Instructions\n",
      "+ Allow Time to **Think**\n",
      "\n",
      " ## Types of Prompts\n",
      "\n",
      "<img src=\"./assets/pe_types.jpg\">\n",
      "\n",
      "### Zero-Shot Prompting\n",
      "Zero-shot or without any examples. Since LLMs are trained on huge amounts of data and instructions, they work pretty well without any specific examples (shots) for usual tasks such as summarization, sentiment classification, grammar checks, etc.\n",
      "\n",
      "_Sample Prompt_:\n",
      "```\n",
      "Classify the text as neutral, positive or negative.\n",
      "Text: The food at this restaurant is so bad.\n",
      "Sentiment:\n",
      "\n",
      "```\n",
      "\n",
      "### Few-Shot Prompting\n",
      "LLMs are good for basic instructions they are trained with but for complex requirements they need some hand-holding or some examples to better understand the instructions.\n",
      "\n",
      "_Sample Prompt_:\n",
      "```\n",
      "Superb drinks and amazing service! > Positive\n",
      "I don't understand why this place is so expensive, worst food ever. > Negative\n",
      "Totally worth it, tasty 100%. > Positive\n",
      "This place is such an utter waste of time. >\n",
      "```\n",
      "**Note**: We did not explicitly instruct our LLM to do sentiment classification, rather gave examples (few-shot) to help it understand\n",
      "\n",
      "\n",
      "### Chain of Thought (COT)\n",
      "Tasks which are more complex and require a bit of *reasoning* (careful there 😉 ) require special measures. Introduced by in a paper of similar title by [Wei et. al.](https://arxiv.org/abs/2201.11903) combines few-shot prompting with additional instructions for the LLM to think through while generating the response.\n",
      "\n",
      "_Sample Prompt_:\n",
      "<img src=\"./assets/cot_few_shot.png\">\n",
      "\n",
      "> Source: [Wei et. al.](https://arxiv.org/abs/2201.11903)\n",
      "\n",
      "#### COT Zero Shot ✨\n",
      "Extension of COT setup where instead of providing examples on how to solve a problem, we explicitly state ``Let's think step by step``. This was introduced by [Kojima et. al.](https://arxiv.org/abs/2205.11916)\n",
      "\n",
      "_Sample Prompt_:\n",
      "```\n",
      "I went to the market and bought 10 apples.\n",
      "I gave 2 apples to the neighbor and 2 to the repairman.\n",
      "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
      "Let's think step by step.\n",
      "```\n",
      "\n",
      "## Advanced Prompting Techniques\n",
      "Prompt Engineering or PE is an active area of research where new techniques\n",
      "are being explored every day. Some of these are:\n",
      "\n",
      "  - [Auto Chain of Thought](https://arxiv.org/abs/2210.03493)\n",
      "  - [Majority Vote or Self-Consistency](https://arxiv.org/abs/2203.11171)\n",
      "  - [Tree of Thoughts](https://arxiv.org/abs/2305.10601)\n",
      "  - Augmented Generation/Retrieval\n",
      "  - [Auto Prompt Engineering (APE)](https://arxiv.org/abs/2211.01910)\n",
      "  - [Multi-modal Prompting](https://arxiv.org/abs/2302.00923)\n",
      "  \n",
      " ## LangChain 🦜🔗\n",
      "- [LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework for developing LLM powered applications.\n",
      "- It provides capabilities to connect LLMs to a number of different sources of data\n",
      "- Provides interfaces for language models to interact with external environment (aka _Agentic_)\n",
      "- Provides for required levels of abstractions to designing end to end applications ## LangChain Conversation Buffer\n",
      "\n",
      "LangChain provides us with an easy to use interface to enable LLMs to refer to context/memory\n",
      "across multiple chains/calls»\n",
      "[3] «# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop/blob/main/module_02/03_simple_text_generator.ipynb\">\n",
      "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
      "</a> ## Get Data\n",
      "We will fine-tune a pre-trained model GPT-2 model on our earlier dataset itself. But wait, what do you mean pre-trained? ## Foundation & Pre-trained Models\n",
      "\n",
      "**Foundation models** are the models that are trained from scratch on a large corpus of data. In the context of NLP, these models are designed to learn the fundamental patterns, structures, and representations of natural language. Foundation models are typically trained using unsupervised learning objectives, such as language modeling or autoencoding, where the model predicts the next word in a sentence or reconstructs the original sentence from a corrupted version/masked version.\n",
      "Models such as GPT, BERT, T5, etc are typical examples of Foundation Models\n",
      "\n",
      "\n",
      "Instances of foundation models that have been trained on specific downstream tasks or datasets are termed as **Pre-Trained Models**. Pretrained models leverage the knowledge learned from foundation models and are fine-tuned on task-specific data to perform well on specific NLP tasks, such as text classification, named entity recognition, machine translation, sentiment analysis, etc. ## Prepare Dataset ## Setup Model Object ## Save the Model ## Decoding Strategies\n",
      "\n",
      "The ``generate()`` utility we used above used every output prediction as input for the next time step. This method of using the highest probability prediction as output is called __Greedy Decoding__. Greeding decoding is fast and simple but is marred with issues we saw in samples we just generated.\n",
      "\n",
      "Focusing on only highest probability output narrows our model's focus to just the next step which inturn may result in inconsistent or non-dictionary terms/words.\n",
      "\n",
      "### Beam Search\n",
      "Beam search is the obvious next step to improve the output predictions from the model. Instead of being greedy, beam search keeps track of n paths at any given time and selects the path with overall higher probability.\n",
      "\n",
      "<img src=\"./assets/beamsearch_nb_2.png\">\n",
      "\n",
      "### Other Key Decoding Strategies:\n",
      "- Sampling\n",
      "- Top-k Sampling\n",
      "- Nucleus Sampling\n",
      "\n",
      "### Temperature\n",
      "Though sampling helps bring in required amount of randomness, it is not free from issues. Random sampling leads to gibberish and incoherence at times. To control the amount of randomness, we introduce __temperature__. This parameter helps increase the likelihood of high probability terms reduce the likelihood of low probability ones. This leads to sharper distributions. \n",
      "\n",
      "> High temperature leads to more randomness while lower temperature brings in predictability.\n",
      " ## Limitations and What Next?\n",
      "- Long Range Context\n",
      "- Scalability\n",
      "- Instruction led generation\n",
      "- Benchmarking\n",
      "- Halucination / Dreaming\n",
      " »\n",
      "\n",
      "Response: Narendra Modi\n",
      "\n",
      "Previous Is Contextual: False\n",
      "\n",
      "Instructions: Response should be factual and based on provided context only\n",
      "\n",
      "Is Contextual:\u001b[32m False\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nCheck if the generated response is from the provided context\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may contain relevant facts\\n\\nResponse: generated response to question\\n\\nPrevious Is Contextual: past Is Contextual: with errors\\n\\nInstructions: Some instructions you must satisfy\\n\\nIs Contextual: generate a boolean response as True if the response is based on context otherwise respond with False\\n\\n---\\n\\nContext:\\n[1] «# Scaling Neural Nets and Efficient Training\\n\\nWe have covered quite some ground in previous 2 modules and observed the steady increase in size and performance of the models. These gains come at huge cost, actual money and human labour apart from time researching and building these things. Can we estimate these costs and draw some insights about model sizes, datasets and comput requirements? ## Estimating Compute Costs\\n> Back of the Envelope Calculations : A quick way to get rough estimates\\n\\n\\n**[LLaMA 3.1](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)** from Meta.AI launched very recently. The model is available in 8B, 70B and 405B sizes and is outperforming a number of existing LLMs on various benchmarks. \\n\\n![image.png](attachment:08264d12-83d1-45ac-8664-90c3f5af5ad6.png) ## So Should We Just Keep Growing?\\n\\n**TL;DR**: Probably not! \\n\\n**Long Answer**: In their work titled [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556) Hoffman et. al. build upon the previous works to showcase that current(_2022_) set of models are **significantly under trained** or the current set of LLMs are far too large for their compute budgets and datasets!\\n\\nThey present a 70B parameter model titled **Chincilla** which was:\\n- 4x smaller than 280B parameter Gopher\\n- trained on 4x more data than Gopher, 1.3T tokens vs 300B tokens\\n\\nand yet **outperformed** Gopher on every task they evaluated!\\n\\n<img src=\"./assets/chinchilla.png\">\\n\\n> Source: [Hoffman et. al.](https://arxiv.org/pdf/2203.15556)\\n> Fine-print: Though undertrained, LLMs increasingly show performance improvement with increasing dataset size ## Ok, So I have a lot of Compute, What\\'s the Problem?\\n\\nThe scaling laws are all good for BigTech, but you could say that most companies have a lot of compute available. Where is the problem? Let us understand this with a simple example walk through\\n\\nAssumptions/Setup:\\n- System RAM (CPU): 32GB\\n- GPU RAM : 32 GB\\n- Model Size : 20B\\n- Parameter Size: 2bytes \\nThis is good for inference but we need to train/fine-tune this model.\\nWe need to accomodate for:\\n- **Gradients/backpropagation** : Size same as model size\\n- **Optimizer States** (ex: ADAM needs momentum and variance, can\\'t be FP16): typically 12x of model size We need more memory (and faster GPUs). But just by usual scaling we would need: ## Can We Bring Some Efficiencies? Please? Yes, luckily researchers have been exploring efficient ways of training and fine-tuning models to democratise availability of such models across a broader spectrum (and lower the environmental impact as well, yay!!!)\\n\\nWe will focus on three key main efficiencies under the umbrella of **P**arameter **E**fficient **F**ine **T**uning paradigm:\\n+ Quantization\\n+ Additive PEFT\\n    + Soft Prompting\\n+ Reparameterization\\n    + LoRA\\n \\nThere are other techniques as well, such as, LoHa, LoKr, IA3, P-Tuning and more. While effective, the basic ideas are similar to what we will cover next. ### Quantization #### Basic Idea\\n- The basic idea is to limit the number of bits/bytes needed to store our weights/parameters/matrix-cell values.\\n- By default we store weights as high-precision floats in float32 or even float64 occupying enormous amounts of space (disk/memory).\\n- Can we be smart and store those values without consuming all 32 or 64 bits?\\n\\n#### Let us understand this with a very simplified example:\\n\\n<img src=\"./assets/quantization.png\"> ### Additive PEFT Additive PEFT works by adding new tunable layers to the model and train only those during fine-tuning while keeping the base model weights frozen. \\n\\nBut how do we do this?\\n\\n#### Soft Prompting\\n- This technique involves introducing **task specific tokens** or **virtual tokens** to the model\\'s input space\\n- The virtual tokens are not part of the actual vocabulary of the model and only specify the task.\\n- The dimensionality of virtual tokens is same as the input token embedding size\\n- During finetuning the base model weights are frozen and only virtual token embedding layer is trained/updated.\\n- Supports Mix-task Batch Fine-Tuning and hence no need for separate heads\\n- Setup is similar to prefix tuning technique\\n\\n#### Overview of Soft Prompting\\n\\n<img src=\"./assets/soft_prompting_1.png\">\\n\\n---\\n\\n<img src=\"./assets/soft_prompting_2.png\">\\n\\n---\\n\\nThis technique is shown to be better/efficient and more stable than **manual prompting**.\\n<img src=\"./assets/soft_prompting_perf.png\">\\n\\n> Source: https://arxiv.org/pdf/2104.08691\\n ## But how much does it cost to train such model(s)?\\n<img src=\"./assets/cost_tweet.png\">\\n\\n> Source: https://x.com/deedydas/status/1629312480165109760\\n\\n__Assumptions__\\nFor the sake of our understanding, we will make the following assumptions:\\n- Ignore costs associated with preparing datasets\\n- Ignore costs associated with training restarts, infra-failures, etc.\\n- Cost of forward and backward pass is set to 1\\n- Assume a very simplified view of overhead associated with multi-GPU/multi-node clusters by setting a standard efficiency ratio (ex: 0.25 efficiency in terms of TFLOPs) ### Reparametrization\\nThis technique smartly leverages matrix decomposition to bring efficiencies. Basically during fine-tuning:\\n- we freeze the base model weights and\\n- during the backward pass, we decompose the **weight update matrix** ($W_\\\\delta$) into two lower rank matrices $W_a$ and $W_b$ of rank $r$\\n- This helps in achieving 100 to 1000x reduction in weights to be updated.\\n\\nThe following illustration explains the same:\\n<img src=\"./assets/lora_1.png\"> > qLoRA combines the improvements of quantization to LoRA thus leading to even further improvements ### Model Parameters\\n- Model Size : 405 **B**illion\\n- Training Dataset : 15 **T**rillion ### Compute Required  ### GPU Performance and Compute Time ### Cost of Training ## Big but How Big?\\n\\nThe latest and the greatest seem to be a thing only the _GPU-rich_ can afford to play with. The exponential increase in the size of models along with their training datasets (we saw GPT vs GPT2 vs GPT3.5 in the previous module) indicates scale is our best friend. \\n\\nWork by Kaplan et. al. in the work titled **[Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361)** presents some interesting takeaways. \\nWe will use the notation from paper as:\\n- **$N$**: Model parameters excluding embeddings\\n- **$D$**: Size of the dataset\\n- **$C$**: Compute used for training the model\\n\\n_Scale is a function of $N$, $D$ and $C$_\\n\\n\\nLet\\'s look at some of the insights from the paper: 1. Performance depends **strongly on scale** and weakly on model shape\\n2. Performance improves predictably as long as we **scale up** **$N$** and **$D$** : \\n_Every time we increase model size 8x, we only need to increase the dataset by roughly 5x_\\n3. Large Models are more **sample efficient** than small models reaching same level of performance with fewer steps and fewer data points <img src=\"./assets/scaling_laws.png\">\\n\\n> Source: [Kaplan et. al.](https://arxiv.org/pdf/2001.08361)»\\n[2] «# Prompt Engineering\\n<img src=\"./assets/pe_banner.jpg\">\\n\\nPrompt Engineering is this thrilling new discipline that opens the door to a world of possibilities with large language models (LLMs).\\n\\nAs a prompt engineer, you\\'ll delve into the depths of LLMs, unraveling their capabilities and limitations with finesse. But prompt engineering isn\\'t about mere prompts. It is aa combination of skills and techniques, enabling you to interact and innovate through the use of LLMs.\\n\\nIn this module, we will step into the fascinating world of prompt engineering, where we will learn about key principals of working with LLMs through prompts.\\n\\n## Local Model using GPT4ALL\\n> GPT4All is an open-source software ecosystem that allows anyone to train and deploy powerful and customized large language models (LLMs) on everyday hardware. Nomic AI oversees contributions to the open-source ecosystem ensuring quality, security and maintainability.\\n\\nIt provides easy to setup and use python bindings.\\n\\n```python\\n!pip install gpt4all\\n```\\n\\nFor OpenAI bindings\\n```python\\n!pip install --upgrade openai\\n``` <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop_dhs23/blob/main/module_04/prompt_engineeering_and_langchain.ipynb\">\\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a> ## Prompting Basics\\n\\n+ Be Clear and Provide Specific Instructions\\n+ Allow Time to **Think**\\n\\n ## Types of Prompts\\n\\n<img src=\"./assets/pe_types.jpg\">\\n\\n### Zero-Shot Prompting\\nZero-shot or without any examples. Since LLMs are trained on huge amounts of data and instructions, they work pretty well without any specific examples (shots) for usual tasks such as summarization, sentiment classification, grammar checks, etc.\\n\\n_Sample Prompt_:\\n```\\nClassify the text as neutral, positive or negative.\\nText: The food at this restaurant is so bad.\\nSentiment:\\n\\n```\\n\\n### Few-Shot Prompting\\nLLMs are good for basic instructions they are trained with but for complex requirements they need some hand-holding or some examples to better understand the instructions.\\n\\n_Sample Prompt_:\\n```\\nSuperb drinks and amazing service! > Positive\\nI don\\'t understand why this place is so expensive, worst food ever. > Negative\\nTotally worth it, tasty 100%. > Positive\\nThis place is such an utter waste of time. >\\n```\\n**Note**: We did not explicitly instruct our LLM to do sentiment classification, rather gave examples (few-shot) to help it understand\\n\\n\\n### Chain of Thought (COT)\\nTasks which are more complex and require a bit of *reasoning* (careful there 😉 ) require special measures. Introduced by in a paper of similar title by [Wei et. al.](https://arxiv.org/abs/2201.11903) combines few-shot prompting with additional instructions for the LLM to think through while generating the response.\\n\\n_Sample Prompt_:\\n<img src=\"./assets/cot_few_shot.png\">\\n\\n> Source: [Wei et. al.](https://arxiv.org/abs/2201.11903)\\n\\n#### COT Zero Shot ✨\\nExtension of COT setup where instead of providing examples on how to solve a problem, we explicitly state ``Let\\'s think step by step``. This was introduced by [Kojima et. al.](https://arxiv.org/abs/2205.11916)\\n\\n_Sample Prompt_:\\n```\\nI went to the market and bought 10 apples.\\nI gave 2 apples to the neighbor and 2 to the repairman.\\nI then went and bought 5 more apples and ate 1. How many apples did I remain with?\\nLet\\'s think step by step.\\n```\\n\\n## Advanced Prompting Techniques\\nPrompt Engineering or PE is an active area of research where new techniques\\nare being explored every day. Some of these are:\\n\\n  - [Auto Chain of Thought](https://arxiv.org/abs/2210.03493)\\n  - [Majority Vote or Self-Consistency](https://arxiv.org/abs/2203.11171)\\n  - [Tree of Thoughts](https://arxiv.org/abs/2305.10601)\\n  - Augmented Generation/Retrieval\\n  - [Auto Prompt Engineering (APE)](https://arxiv.org/abs/2211.01910)\\n  - [Multi-modal Prompting](https://arxiv.org/abs/2302.00923)\\n  \\n ## LangChain 🦜🔗\\n- [LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework for developing LLM powered applications.\\n- It provides capabilities to connect LLMs to a number of different sources of data\\n- Provides interfaces for language models to interact with external environment (aka _Agentic_)\\n- Provides for required levels of abstractions to designing end to end applications ## LangChain Conversation Buffer\\n\\nLangChain provides us with an easy to use interface to enable LLMs to refer to context/memory\\nacross multiple chains/calls»\\n[3] «# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop/blob/main/module_02/03_simple_text_generator.ipynb\">\\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a> ## Get Data\\nWe will fine-tune a pre-trained model GPT-2 model on our earlier dataset itself. But wait, what do you mean pre-trained? ## Foundation & Pre-trained Models\\n\\n**Foundation models** are the models that are trained from scratch on a large corpus of data. In the context of NLP, these models are designed to learn the fundamental patterns, structures, and representations of natural language. Foundation models are typically trained using unsupervised learning objectives, such as language modeling or autoencoding, where the model predicts the next word in a sentence or reconstructs the original sentence from a corrupted version/masked version.\\nModels such as GPT, BERT, T5, etc are typical examples of Foundation Models\\n\\n\\nInstances of foundation models that have been trained on specific downstream tasks or datasets are termed as **Pre-Trained Models**. Pretrained models leverage the knowledge learned from foundation models and are fine-tuned on task-specific data to perform well on specific NLP tasks, such as text classification, named entity recognition, machine translation, sentiment analysis, etc. ## Prepare Dataset ## Setup Model Object ## Save the Model ## Decoding Strategies\\n\\nThe ``generate()`` utility we used above used every output prediction as input for the next time step. This method of using the highest probability prediction as output is called __Greedy Decoding__. Greeding decoding is fast and simple but is marred with issues we saw in samples we just generated.\\n\\nFocusing on only highest probability output narrows our model\\'s focus to just the next step which inturn may result in inconsistent or non-dictionary terms/words.\\n\\n### Beam Search\\nBeam search is the obvious next step to improve the output predictions from the model. Instead of being greedy, beam search keeps track of n paths at any given time and selects the path with overall higher probability.\\n\\n<img src=\"./assets/beamsearch_nb_2.png\">\\n\\n### Other Key Decoding Strategies:\\n- Sampling\\n- Top-k Sampling\\n- Nucleus Sampling\\n\\n### Temperature\\nThough sampling helps bring in required amount of randomness, it is not free from issues. Random sampling leads to gibberish and incoherence at times. To control the amount of randomness, we introduce __temperature__. This parameter helps increase the likelihood of high probability terms reduce the likelihood of low probability ones. This leads to sharper distributions. \\n\\n> High temperature leads to more randomness while lower temperature brings in predictability.\\n ## Limitations and What Next?\\n- Long Range Context\\n- Scalability\\n- Instruction led generation\\n- Benchmarking\\n- Halucination / Dreaming\\n »\\n\\nResponse: Narendra Modi\\n\\nPrevious Is Contextual: False\\n\\nInstructions: Response should be factual and based on provided context only\\n\\nIs Contextual:\\x1b[32m False\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4c4e9-2adb-465e-9594-3bfb87a34885",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Once we have assertions in place, we need to develop an evaluation metric like ``number_of_outofcontext_responses`` which can be a simple average of cases where assertion fails\n",
    "- Prepare a golden few-shot dataset\n",
    "- Fine-tune/distill a student model (without assertions) using a teacher model (with assertions) to improve the overall pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b16b5-d6e4-44c1-889f-370f398ebd08",
   "metadata": {},
   "source": [
    "## How Does it All Go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbff9178-1c78-4a2b-968f-aa560d8ad29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4a340b4-39f4-477e-bf26-4b9b3951b0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question**:Which model is used for instruction fine-tuning?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: LLaMA"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instruction Tuning with Optimizations\n",
      "\n",
      "Instruction tuning is form of fine-tuning that enhances a m...\n",
      "# Quick Overview of RLFH\n",
      "\n",
      "The performance of Language Models until GPT-3 was kind of amazing as-is. ...\n",
      "# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_w...\n",
      "# Getting Started : Text Representation\n",
      "<img src=\"./assets/banner_notebook_1.jpg\">\n",
      "\n",
      "\n",
      "The NLP domain ...\n",
      "# <img src=\"./assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
      "---\n",
      "<img src=\"./assets/dspy_b...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:List the models covered in module03"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: GPT, BERT, T5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Prompt Engineering\n",
      "<img src=\"./assets/pe_banner.jpg\">\n",
      "\n",
      "Prompt Engineering is this thrilling new di...\n",
      "# Open Source Vs Close Sourced LLMs\n",
      "\n",
      "Similar to any other piece of technology, LLMs are available in...\n",
      "# <img src=\"./assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
      "---\n",
      "<img src=\"./assets/dspy_b...\n",
      "# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_w...\n",
      "# Scaling Neural Nets and Efficient Training\n",
      "\n",
      "We have covered quite some ground in previous 2 module...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:Summarize key takeways for module02"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: Answer: Text generation techniques"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Prompt Engineering\n",
      "<img src=\"./assets/pe_banner.jpg\">\n",
      "\n",
      "Prompt Engineering is this thrilling new di...\n",
      "# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_w...\n",
      "# Getting Started : Text Representation\n",
      "<img src=\"./assets/banner_notebook_1.jpg\">\n",
      "\n",
      "\n",
      "The NLP domain ...\n",
      "## Exploring Transformer Architectures ## The RNN Limitation\n",
      "The RNN layer (LSTM, or GRU, etc.) take...\n",
      "# Retrieval Augmented LLM App\n",
      "<img src=\"./assets/rap_banner.jpeg\">\n",
      "\n",
      "We have covered quite some groun...\n",
      "# Quick Overview of RLFH\n",
      "\n",
      "The performance of Language Models until GPT-3 was kind of amazing as-is. ...\n",
      "# <img src=\"./assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
      "---\n",
      "<img src=\"./assets/dspy_b...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:What is the focus of the following modules: module_01,module_02,module_03 and module_04? Respond as a list"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: 1. Text Representation\n",
       "2. Text Generation\n",
       "3. Instruction Tuning\n",
       "4. Prompt Engineering"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Prompt Engineering\n",
      "<img src=\"./assets/pe_banner.jpg\">\n",
      "\n",
      "Prompt Engineering is this thrilling new di...\n",
      "# <img src=\"./assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
      "---\n",
      "<img src=\"./assets/dspy_b...\n",
      "# Getting Started : Text Representation\n",
      "<img src=\"./assets/banner_notebook_1.jpg\">\n",
      "\n",
      "\n",
      "The NLP domain ...\n",
      "# Instruction Tuning with Optimizations\n",
      "\n",
      "Instruction tuning is form of fine-tuning that enhances a m...\n",
      "# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_w...\n",
      "# Open Source Vs Close Sourced LLMs\n",
      "\n",
      "Similar to any other piece of technology, LLMs are available in...\n",
      "# Retrieval Augmented LLM App\n",
      "<img src=\"./assets/rap_banner.jpeg\">\n",
      "\n",
      "We have covered quite some groun...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:For RLHF what policy is covered in module03?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: Proximal Policy Optimization (PPO)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Quick Overview of RLFH\n",
      "\n",
      "The performance of Language Models until GPT-3 was kind of amazing as-is. ...\n",
      "# Prompt Engineering\n",
      "<img src=\"./assets/pe_banner.jpg\">\n",
      "\n",
      "Prompt Engineering is this thrilling new di...\n",
      "# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_w...\n",
      "# Getting Started : Text Representation\n",
      "<img src=\"./assets/banner_notebook_1.jpg\">\n",
      "\n",
      "\n",
      "The NLP domain ...\n",
      "# <img src=\"./assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
      "---\n",
      "<img src=\"./assets/dspy_b...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Which model is used for instruction fine-tuning?\",\n",
    "    \"List the models covered in module03\",\n",
    "    \"Summarize key takeways for module02\",\n",
    "    \"What is the focus of the following modules: module_01,module_02,module_03 and module_04? Respond as a list\",\n",
    "    \"For RLHF what policy is covered in module03?\"\n",
    "]\n",
    "\n",
    "uncompiled_baleen = SimplifiedBaleen(passages_per_hop=5)  # uncompiled (i.e., zero-shot) program\n",
    "for question in questions:\n",
    "    display(Markdown(f\"**Question**:{question}\"))\n",
    "    pred = uncompiled_baleen(question)\n",
    "    display(Markdown(f\"**Predicted Answer**: {pred.answer}\"))\n",
    "    display(Markdown(\"**Retrieved Contexts (truncated)**:\"))\n",
    "    for c in pred.context:\n",
    "        print(f\"{c[:100]}...\" )\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696c37a-0f5a-435f-8e67-e1e9885c1a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
