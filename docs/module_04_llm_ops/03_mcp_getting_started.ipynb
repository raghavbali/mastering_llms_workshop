{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b2999f-4059-46ee-bcb2-faf15d93f6b4",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop/blob/main/docs/module_04_llm_ops/03_mcp_getting_started.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f28c9cb-acc1-49a4-8c52-cccf813eb2a0",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP) \n",
    "### Why MCP\n",
    "LLMs are powerful but limited to their training data and can't interact with external systems, databases, or real-time information. MCP servers bridge this gap by providing secure, standardized connections between LLMs and external resources.\n",
    "\n",
    "### What is MCP?\n",
    "An **MCP (Model Context Protocol)** server is a lightweight service that exposes specific capabilities—like database queries, API calls, file operations, or tool integrations—to LLMs through a standardized protocol. It acts as a secure intermediary that allows AI models to access and interact with external systems while maintaining proper authentication and access controls.\n",
    "\n",
    "> Think of MCP like a USB-C port for AI applications\n",
    "\n",
    "\n",
    "<img src=\"../assets/04_mcp.png\">\n",
    "\n",
    "> Source: [IBM](https://www.ibm.com/think/topics/model-context-protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82541fb5-7063-463a-b0a9-926f0a0ec547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install fastmcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d9d3b63-51d2-49ad-867a-8d7169b6cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notebook_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile notebook_server.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "from fastmcp import FastMCP\n",
    "from scraper_utils import NB_Markdown_Scraper\n",
    "\n",
    "mcp = FastMCP(\n",
    "    name=\"Notebook Server\",\n",
    "    instructions=\"\"\"\n",
    "        This server provides a markdown scraper utility\n",
    "        and a tool to write JSON file\n",
    "        \"\"\",)\n",
    "\n",
    "class NotebookServer():\n",
    "    def __init__(self,mcp_instance):\n",
    "        self.notebook_scraper = NB_Markdown_Scraper(input_paths=[f'../{d}' for d in os.listdir(\"../\") if d.startswith(\"module\")])\n",
    "        \n",
    "        # Register methods  \n",
    "        mcp_instance.tool(self.greet)\n",
    "        mcp_instance.tool(self.get_notebook_list)\n",
    "        mcp_instance.tool(self.get_markdown_from_notebook)\n",
    "        mcp_instance.tool(self.notebook_scraper.scrape_markdowns)\n",
    "        mcp_instance.tool(self.write_json)\n",
    "        mcp_instance.resource(\"resource://data\")(self.resource_method)\n",
    "\n",
    "    def greet(self,name: str= None):\n",
    "        '''Greets the User'''\n",
    "        if not name:\n",
    "            return \"Hi, I am NotebookServer\"\n",
    "        else:\n",
    "            return f\"Hi {name}, I am NotebookServer\"\n",
    "\n",
    "    def get_notebook_list(self):\n",
    "        '''Returns List of Notebooks Scraped'''\n",
    "        return list(self.notebook_scraper.notebook_md_dict.keys())\n",
    "\n",
    "    def get_markdown_from_notebook(self,notebook_name):\n",
    "        '''Returns Markdown Cells for specified notebook'''\n",
    "        if notebook_name in list(self.notebook_scraper.notebook_md_dict.keys()):\n",
    "            return self.notebook_scraper.notebook_md_dict[notebook_name]\n",
    "        else:\n",
    "            return f\"Requested notebook ({notebook_name}) does not exist\"\n",
    "        \n",
    "    def write_json(self,file_name: str):\n",
    "        '''Tool to write a json file in the format notebook:markdown content'''\n",
    "        try:\n",
    "            with open(f\"./{file_name}\", \"w\") as record_file:\n",
    "                json.dump(self.notebook_scraper.notebook_md_dict,record_file)\n",
    "            return f\"File:{file_name} written successfully\"\n",
    "        except Exception as ex:\n",
    "            return f\"Could not write {file_name} due to {ex}\"\n",
    "        \n",
    "    \n",
    "    def resource_method(self):\n",
    "        return \"\"\"\n",
    "        Resources provide read-only access to data for the LLM or client application. When a client requests a resource URI:\n",
    "            + FastMCP finds the corresponding resource definition.\n",
    "            + If it’s dynamic (defined by a function), the function is executed.\n",
    "            + The content (text, JSON, binary data) is returned to the client.\n",
    "        This allows LLMs to access files, database content, configuration, or dynamically generated information relevant to the conversation.\n",
    "        \"\"\"\n",
    "\n",
    "# The methods are automatically registered when creating the instance\n",
    "provider = NotebookServer(mcp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize and run the server\n",
    "    mcp.run(transport='stdio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9995e7d-5ba9-41c3-8e41-1b47271b9117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dummy_client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dummy_client.py\n",
    "import asyncio\n",
    "from fastmcp import Client, FastMCP\n",
    "\n",
    "# In-memory server (ideal for testing)\n",
    "server = FastMCP(\"TestServer\")\n",
    "client = Client(server)\n",
    "\n",
    "# HTTP server\n",
    "client = Client(\"https://example.com/mcp\")\n",
    "\n",
    "# Local Python script\n",
    "client = Client(\"notebook_server.py\")\n",
    "\n",
    "async def main():\n",
    "    async with client:\n",
    "        # Basic server interaction\n",
    "        await client.ping()\n",
    "        \n",
    "        # List available operations\n",
    "        tools = await client.list_tools()\n",
    "        resources = await client.list_resources()\n",
    "        prompts = await client.list_prompts()\n",
    "        \n",
    "        print(\"-\"*30)\n",
    "        print(\"Tools:\")\n",
    "        print(\"-\"*30)\n",
    "        print(tools)\n",
    "        print(\"-\"*30)\n",
    "        print(\"Resources:\")\n",
    "        print(\"-\"*30)\n",
    "        print(resources)\n",
    "        print(\"-\"*30)\n",
    "        print(\"Prompts:\")\n",
    "        print(prompts)\n",
    "        print(\"-\"*30)\n",
    "        # Execute operations\n",
    "        await client.call_tool(\"scrape_markdowns\", {})\n",
    "        result = await client.call_tool(\"write_json\", {\"file_name\":\"test_mcp_server.json\"})\n",
    "        print(\"-\"*30)\n",
    "        print(f\"Result of tool call for write_json:\\n{result}\")\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea911f33-6f7f-45c4-a98d-cf2ad89e60fd",
   "metadata": {},
   "source": [
    "> Go to terminal and from the module 4's directory execute : ``>python3 dummy_client.py``\n",
    "\n",
    "The following should be the output on your screen\n",
    "<img src=\"../assets/04_mcp_client.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e74a7-bb6e-47b7-afc4-b5669ad09700",
   "metadata": {},
   "source": [
    "## ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f343c5c-42f8-458e-b9ca-3da3069b3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_chatbot.py\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from ollama import Client\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.session: ClientSession = None\n",
    "        self.available_tools: List[dict] = []\n",
    "        self.open_ai_compat_client = Client(\n",
    "            host='http://localhost:11434',\n",
    "            # headers={'x-some-header': 'some-value'}\n",
    "        )\n",
    "        self.model_name = 'llama3.1'\n",
    "\n",
    "    async def process_query(self, query):\n",
    "        messages = [{'role':'user', 'content':query}]\n",
    "        response = self.open_ai_compat_client.chat(model=self.model_name, messages=messages,tools=self.available_tools)\n",
    "        process_query = True\n",
    "        while process_query:\n",
    "            assistant_content = []\n",
    "            # for content in response.message.content:\n",
    "            content = response.message.content\n",
    "            tool_calls = response.message.tool_calls\n",
    "            print(content)\n",
    "            if not tool_calls:\n",
    "                print(\"No tool calls detected\")\n",
    "                assistant_content.append(content)\n",
    "                if(len(response.message.content) >1):\n",
    "                    process_query= False\n",
    "            elif tool_calls:\n",
    "                print(\" tool calls detected\")\n",
    "                assistant_content.append(content)\n",
    "                messages.append({'role':'assistant', 'content':assistant_content})\n",
    "                tool_args = tool_calls.function.arguments\n",
    "                tool_name = tool_calls.function.name\n",
    "\n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                \n",
    "                # Call a tool\n",
    "                # tool invocation through the client session\n",
    "                result = await self.session.call_tool(tool_name, arguments=tool_args)\n",
    "                messages.append({\"role\": \"user\", \n",
    "                                  \"message\": [\n",
    "                                      {\n",
    "                                          # \"tool_use_id\":tool_id,\n",
    "                                          \"content\": result.message.content\n",
    "                                      }\n",
    "                                  ]\n",
    "                                })\n",
    "                response = self.open_ai_compat_client.chat(model=self.model_name, messages=messages,tools=self.available_tools)                \n",
    "                if not response.message.tool_calls:\n",
    "                    print(response.message.content)\n",
    "                    process_query= False\n",
    "\n",
    "    \n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "        \n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                    \n",
    "                await self.process_query(query)\n",
    "                print(\"\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "    \n",
    "    async def connect_to_server_and_run(self):\n",
    "        # Create server parameters for stdio connection\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"python3\",  # Executable\n",
    "            args=[\"notebook_server.py\"],  # Optional command line arguments\n",
    "            env=None,  # Optional environment variables\n",
    "        )\n",
    "        async with stdio_client(server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                self.session = session\n",
    "                # Initialize the connection\n",
    "                await session.initialize()\n",
    "    \n",
    "                # List available tools\n",
    "                response = await session.list_tools()\n",
    "                \n",
    "                tools = response.tools\n",
    "                print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "                \n",
    "                self.available_tools = [{\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"input_schema\": tool.inputSchema\n",
    "                } for tool in response.tools]\n",
    "    \n",
    "                await self.chat_loop()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    await chatbot.connect_to_server_and_run()\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cadcbd53-d756-4af6-8082-0e6536f96657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_chatbot_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_chatbot_v2.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, TypedDict\n",
    "from contextlib import AsyncExitStack\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: dict\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.sessions: List[ClientSession] = []\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        \n",
    "        # Initialize OpenAI client with configurable base URL for Ollama compatibility\n",
    "        base_url = os.getenv(\"OPENAI_BASE_URL\", 'http://localhost:11434')#\"https://api.openai.com/v1\")\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\", \"\")  # Ollama doesn't need a real API key\n",
    "        \n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=api_key\n",
    "        )\n",
    "        \n",
    "        # Model configuration\n",
    "        self.model = os.getenv(\"MODEL_NAME\", \"llama3.1\")  # Default to OpenAI, can be changed to ollama model\n",
    "        \n",
    "        self.available_tools: List[ToolDefinition] = []\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {}\n",
    "\n",
    "    def convert_mcp_tools_to_openai_format(self) -> List[Dict]:\n",
    "        \"\"\"Convert MCP tool definitions to OpenAI function calling format.\"\"\"\n",
    "        openai_tools = []\n",
    "        for tool in self.available_tools:\n",
    "            openai_tool = {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool[\"name\"],\n",
    "                    \"description\": tool[\"description\"],\n",
    "                    \"parameters\": tool[\"input_schema\"]\n",
    "                }\n",
    "            }\n",
    "            openai_tools.append(openai_tool)\n",
    "        return openai_tools\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_config: dict) -> None:\n",
    "        \"\"\"Connect to a single MCP server.\"\"\"\n",
    "        try:\n",
    "            # server_params = StdioServerParameters(**server_config)\n",
    "            server_params = StdioServerParameters(\n",
    "                command=\"python3\",  # Executable\n",
    "                args=[\"notebook_server.py\"],  # Optional command line arguments\n",
    "                env=None,  # Optional environment variables\n",
    "            )\n",
    "            stdio_transport = await self.exit_stack.enter_async_context(\n",
    "                stdio_client(server_params)\n",
    "            )\n",
    "            read, write = stdio_transport\n",
    "            session = await self.exit_stack.enter_async_context(\n",
    "                ClientSession(read, write)\n",
    "            )\n",
    "            await session.initialize()\n",
    "            self.sessions.append(session)\n",
    "            \n",
    "            # List available tools for this session\n",
    "            response = await session.list_tools()\n",
    "            tools = response.tools\n",
    "            print(f\"\\nConnected to {server_name} with tools:\", [t.name for t in tools])\n",
    "            \n",
    "            for tool in tools:\n",
    "                self.tool_to_session[tool.name] = session\n",
    "                self.available_tools.append({\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"input_schema\": tool.inputSchema\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    # async def connect_to_servers(self):\n",
    "    #     \"\"\"Connect to all configured MCP servers.\"\"\"\n",
    "    #     try:\n",
    "    #         with open(\"server_config.json\", \"r\") as file:\n",
    "    #             data = json.load(file)\n",
    "            \n",
    "    #         servers = data.get(\"mcpServers\", {})\n",
    "            \n",
    "    #         for server_name, server_config in servers.items():\n",
    "    #             await self.connect_to_server(server_name, server_config)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error loading server configuration: {e}\")\n",
    "    #         raise\n",
    "    \n",
    "    async def process_query(self, query):\n",
    "        messages = [{'role': 'user', 'content': query}]\n",
    "        \n",
    "        # Convert MCP tools to OpenAI format\n",
    "        openai_tools = self.convert_mcp_tools_to_openai_format()\n",
    "        \n",
    "        # Make initial API call\n",
    "        kwargs = {\n",
    "            'model': self.model,\n",
    "            'messages': messages,\n",
    "            'max_tokens': 2024\n",
    "        }\n",
    "        \n",
    "        # Only add tools if we have any available\n",
    "        if openai_tools:\n",
    "            kwargs['tools'] = openai_tools\n",
    "            kwargs['tool_choice'] = 'auto'\n",
    "        \n",
    "        response = self.client.chat.completions.create(**kwargs)\n",
    "        \n",
    "        process_query = True\n",
    "        while process_query:\n",
    "            message = response.choices[0].message\n",
    "            \n",
    "            # Handle text response\n",
    "            if message.content:\n",
    "                print(message.content)\n",
    "                \n",
    "            # Handle tool calls\n",
    "            if message.tool_calls:\n",
    "                # Add assistant message with tool calls to conversation\n",
    "                messages.append({\n",
    "                    'role': 'assistant',\n",
    "                    'content': message.content,\n",
    "                    'tool_calls': message.tool_calls\n",
    "                })\n",
    "                \n",
    "                # Process each tool call\n",
    "                for tool_call in message.tool_calls:\n",
    "                    tool_name = tool_call.function.name\n",
    "                    tool_args = json.loads(tool_call.function.arguments)\n",
    "                    tool_call_id = tool_call.id\n",
    "                    \n",
    "                    print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                    \n",
    "                    # Call the MCP tool\n",
    "                    session = self.tool_to_session[tool_name]\n",
    "                    result = await session.call_tool(tool_name, arguments=tool_args)\n",
    "                    \n",
    "                    # Add tool result to messages\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call_id,\n",
    "                        \"content\": str(result.content)\n",
    "                    })\n",
    "                \n",
    "                # Make follow-up API call with tool results\n",
    "                kwargs = {\n",
    "                    'model': self.model,\n",
    "                    'messages': messages,\n",
    "                    'max_tokens': 2024\n",
    "                }\n",
    "                \n",
    "                if openai_tools:\n",
    "                    kwargs['tools'] = openai_tools\n",
    "                    kwargs['tool_choice'] = 'auto'\n",
    "                \n",
    "                response = self.client.chat.completions.create(**kwargs)\n",
    "            else:\n",
    "                # No tool calls, we're done\n",
    "                process_query = False\n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(f\"\\nMCP Chatbot Started! Using model: {self.model}\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "        \n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                    \n",
    "                await self.process_query(query)\n",
    "                print(\"\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Cleanly close all resources using AsyncExitStack.\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        await chatbot.connect_to_server(\"Notebook Server\",None)\n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41aea09-b4f0-4b0d-9a94-16dc1cc5c120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
