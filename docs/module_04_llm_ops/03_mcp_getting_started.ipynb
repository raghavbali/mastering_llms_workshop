{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b2999f-4059-46ee-bcb2-faf15d93f6b4",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop/blob/main/docs/module_04_llm_ops/03_mcp_getting_started.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c74ac-77bf-4d6a-a062-eb5462b16753",
   "metadata": {},
   "source": [
    "# Tool Calling & MCP \n",
    "\n",
    "<img src=\"../assets/04_tool_calling.png\">\n",
    "\n",
    "> Source: Generated with ❤️ Gemini 2.5 Flash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a2633-4755-4b5d-a4cd-1d9085d6631c",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "\n",
    "Tool calling, or function calling, is a feature that allows LLMs to interact with external systems. Rather than just generating text, an LLM can be given a list of tools (like APIs or functions) it can use.\n",
    "\n",
    "### How Does Tool Calling Work?\n",
    "When a user provides a prompt, the LLM determines if a tool is needed. If so, it outputs a structured request (like a JSON object) detailing which tool to call and with what arguments. A separate program then executes this tool and returns the result to the LLM. The LLM uses this real-time information to formulate a more accurate and comprehensive final response, effectively extending its capabilities beyond its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "970d101a-a868-45ac-a451-0b44c9b25dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Add two numbers\n",
    "\n",
    "  Args:\n",
    "    a: The first integer number\n",
    "    b: The second integer number\n",
    "\n",
    "  Returns:\n",
    "    int: The sum of the two numbers\n",
    "  \"\"\"\n",
    "  return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc637c27-4faa-4c31-992d-dcfac16cc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "  'llama3.1',\n",
    "  messages=[{'role': 'user', 'content': 'What is two plus three?'}],\n",
    "  tools=[add_two_numbers], # Actual function reference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bfb9c700-b612-4d20-b2b8-dd6f88b882f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function output: 5\n"
     ]
    }
   ],
   "source": [
    "available_functions = {\n",
    "  'add_two_numbers': add_two_numbers,\n",
    "}\n",
    "\n",
    "for tool in response.message.tool_calls or []:\n",
    "  function_to_call = available_functions.get(tool.function.name)\n",
    "  if function_to_call:\n",
    "    print('Function output:', function_to_call(**tool.function.arguments))\n",
    "  else:\n",
    "    print('Function not found:', tool.function.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f96ca90f-b279-4ac2-b495-5d6ea80d4db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='llama3.1', created_at='2025-08-05T20:15:18.775024Z', done=True, done_reason='stop', total_duration=1083238250, load_duration=44108833, prompt_eval_count=166, prompt_eval_duration=204444083, eval_count=24, eval_duration=834210500, message=Message(role='assistant', content='', thinking=None, images=None, tool_calls=[ToolCall(function=Function(name='add_two_numbers', arguments={'a': 2, 'b': 3}))]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f28c9cb-acc1-49a4-8c52-cccf813eb2a0",
   "metadata": {},
   "source": [
    "## Model Context Protocol (MCP) \n",
    "\n",
    "<img src=\"../assets/04_llm_mcp.png\">\n",
    "\n",
    "> Source: Generated with ❤️ Gemini 2.5 Flash\n",
    "\n",
    "### Why MCP\n",
    "LLMs are powerful but limited to their training data and can't interact with external systems, databases, or real-time information. MCP servers bridge this gap by providing secure, standardized connections between LLMs and external resources.\n",
    "\n",
    "### What is MCP?\n",
    "An **MCP (Model Context Protocol)** server is a lightweight service that exposes specific capabilities—like database queries, API calls, file operations, or tool integrations—to LLMs through a standardized protocol. It acts as a secure intermediary that allows AI models to access and interact with external systems while maintaining proper authentication and access controls.\n",
    "\n",
    "> Think of MCP like a USB-C port for AI applications\n",
    "\n",
    "\n",
    "<img src=\"../assets/04_mcp.png\">\n",
    "\n",
    "> Source: [IBM](https://www.ibm.com/think/topics/model-context-protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82541fb5-7063-463a-b0a9-926f0a0ec547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install fastmcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2c960-19c6-480a-8f04-5e66715387c7",
   "metadata": {},
   "source": [
    "### MCP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d9d3b63-51d2-49ad-867a-8d7169b6cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notebook_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile notebook_server.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "from fastmcp import FastMCP\n",
    "from scraper_utils import NB_Markdown_Scraper\n",
    "\n",
    "mcp = FastMCP(\n",
    "    name=\"Notebook Server\",\n",
    "    instructions=\"\"\"\n",
    "        This server provides a markdown scraper utility\n",
    "        and a tool to write JSON file\n",
    "        \"\"\",)\n",
    "\n",
    "class NotebookServer():\n",
    "    def __init__(self,mcp_instance):\n",
    "        self.notebook_scraper = NB_Markdown_Scraper(input_paths=[f'../{d}' for d in os.listdir(\"../\") if d.startswith(\"module\")])\n",
    "        \n",
    "        # Register methods  \n",
    "        mcp_instance.tool(self.greet)\n",
    "        mcp_instance.tool(self.get_notebook_list)\n",
    "        mcp_instance.tool(self.get_markdown_from_notebook)\n",
    "        mcp_instance.tool(self.notebook_scraper.scrape_markdowns)\n",
    "        mcp_instance.tool(self.write_json)\n",
    "        mcp_instance.resource(\"resource://data\")(self.resource_method)\n",
    "\n",
    "    def greet(self,name: str= None):\n",
    "        '''Greets the User'''\n",
    "        if not name:\n",
    "            return \"Hi, I am NotebookServer\"\n",
    "        else:\n",
    "            return f\"Hi {name}, I am NotebookServer\"\n",
    "\n",
    "    def get_notebook_list(self):\n",
    "        '''Returns List of Notebooks Scraped'''\n",
    "        return list(self.notebook_scraper.notebook_md_dict.keys())\n",
    "\n",
    "    def get_markdown_from_notebook(self,notebook_name):\n",
    "        '''Returns Markdown Cells for specified notebook'''\n",
    "        if notebook_name in list(self.notebook_scraper.notebook_md_dict.keys()):\n",
    "            return self.notebook_scraper.notebook_md_dict[notebook_name]\n",
    "        else:\n",
    "            return f\"Requested notebook ({notebook_name}) does not exist\"\n",
    "        \n",
    "    def write_json(self,file_name: str):\n",
    "        '''Tool to write a json file in the format notebook:markdown content'''\n",
    "        try:\n",
    "            with open(f\"./{file_name}\", \"w\") as record_file:\n",
    "                json.dump(self.notebook_scraper.notebook_md_dict,record_file)\n",
    "            return f\"File:{file_name} written successfully\"\n",
    "        except Exception as ex:\n",
    "            return f\"Could not write {file_name} due to {ex}\"\n",
    "        \n",
    "    \n",
    "    def resource_method(self):\n",
    "        return \"\"\"\n",
    "        Resources provide read-only access to data for the LLM or client application. When a client requests a resource URI:\n",
    "            + FastMCP finds the corresponding resource definition.\n",
    "            + If it’s dynamic (defined by a function), the function is executed.\n",
    "            + The content (text, JSON, binary data) is returned to the client.\n",
    "        This allows LLMs to access files, database content, configuration, or dynamically generated information relevant to the conversation.\n",
    "        \"\"\"\n",
    "\n",
    "# The methods are automatically registered when creating the instance\n",
    "provider = NotebookServer(mcp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize and run the server\n",
    "    mcp.run(transport='stdio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73933bf-7252-4bd2-90c7-15faba226f51",
   "metadata": {},
   "source": [
    "### MCP Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5972b46-5d9e-4b45-ad08-964b508ff179",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dummy_client.py\n",
    "import asyncio\n",
    "from fastmcp import Client, FastMCP\n",
    "\n",
    "# In-memory server (ideal for testing)\n",
    "server = FastMCP(\"TestServer\")\n",
    "client = Client(server)\n",
    "\n",
    "# HTTP server\n",
    "client = Client(\"https://example.com/mcp\")\n",
    "\n",
    "# Local Python script\n",
    "client = Client(\"notebook_server.py\")\n",
    "\n",
    "async def main():\n",
    "    async with client:\n",
    "        # Basic server interaction\n",
    "        await client.ping()\n",
    "        \n",
    "        # List available operations\n",
    "        tools = await client.list_tools()\n",
    "        resources = await client.list_resources()\n",
    "        prompts = await client.list_prompts()\n",
    "        \n",
    "        print(\"-\"*30)\n",
    "        print(\"Tools:\")\n",
    "        print(\"-\"*30)\n",
    "        print(tools)\n",
    "        print(\"-\"*30)\n",
    "        print(\"Resources:\")\n",
    "        print(\"-\"*30)\n",
    "        print(resources)\n",
    "        print(\"-\"*30)\n",
    "        print(\"Prompts:\")\n",
    "        print(prompts)\n",
    "        print(\"-\"*30)\n",
    "        # Execute operations\n",
    "        await client.call_tool(\"scrape_markdowns\", {})\n",
    "        result = await client.call_tool(\"write_json\", {\"file_name\":\"test_mcp_server.json\"})\n",
    "        print(\"-\"*30)\n",
    "        print(f\"Result of tool call for write_json:\\n{result}\")\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea911f33-6f7f-45c4-a98d-cf2ad89e60fd",
   "metadata": {},
   "source": [
    "> Go to terminal and from the module 4's directory execute : ``>python3 dummy_client.py``\n",
    "\n",
    "The following should be the output on your screen\n",
    "<img src=\"../assets/04_mcp_client.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fd92d-363f-4447-a88d-b709f4de35fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
