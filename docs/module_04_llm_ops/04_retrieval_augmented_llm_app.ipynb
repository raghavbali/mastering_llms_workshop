{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ7lJK0y9gJO"
   },
   "source": [
    "# Retrieval Augmented LLM App\n",
    "<img src=\"./assets/rap_banner.jpeg\">\n",
    "\n",
    "We have covered quite some ground in terms of understanding and building components for:\n",
    "- Text Representation\n",
    "- NLP Tasks\n",
    "- Pretrained Models and Transfer Learning\n",
    "- Model Fine-Tuning PEFT\n",
    "- SFT and LLM Landscape\n",
    "- Vector Databases\n",
    "- Libraries and Frameworks\n",
    "\n",
    "Now we will work through development of an app to showcase how we can leverage all the concepts into a fully functioning system\n",
    "\n",
    "__Note__: In order to keep things simple, we will leverage most high-level APIs available but the overall setup should be easily extensible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFRjKaov9gJR"
   },
   "source": [
    "## Why Retrieval Augmentation\n",
    "\n",
    "While theoretically LLMs are capable of having super long context windows, in real world settings this is a challenge because of:\n",
    "- Inability/Limitation to ensure LLM focusses on correct sub-sections of the context\n",
    "- High Memory requirements\n",
    "- High API Cost\n",
    "- High Latency , etc.\n",
    "\n",
    "\n",
    "In order to overcome such challenges, we leverage vector databases to act as intelligent retrieval systems (again powered by LLMs) to:\n",
    "- Provide focussed context\n",
    "- Reduce memory, cost and latency requirements\n",
    "- Unlock super-abilities to use upto-date information\n",
    "- Offload trivial tasks to expert systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxi-yR1n9gJR"
   },
   "source": [
    "## Streamlit Enters the Arena\n",
    "\n",
    "[Streamlit](https://streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop/blob/main/module_04/04_retrieval_augmented_llm_app.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTFt0rOY9gJR"
   },
   "source": [
    "## Let us Begin with Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z_dNazilzRUF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# install dependencies\n",
    "# !pip install -q chromadb\n",
    "# !pip install retry\n",
    "# !pip install -q streamlit \n",
    "# !npm install localtunnel # this is needed if you are working from colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjSlib36bJ70",
    "outputId": "9ccfe431-7738-459d-9d4f-b977c7fc7a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "## import required components\n",
    "\n",
    "import pandas as pd\n",
    "from utils import (\n",
    "    get_lines,\n",
    "    load_data,\n",
    "    get_relevant_documents,\n",
    "    get_answer,\n",
    "    create_db,\n",
    "    sidebar,\n",
    ")\n",
    "import streamlit as st\n",
    "chroma_client, db = create_db()\n",
    "\n",
    "## Setup Page Header and Sidebar\n",
    "st.set_page_config(page_title=\"PersonalGPT\", page_icon=\"📖\", layout=\"wide\")\n",
    "lm_model = sidebar()\n",
    "st.header(f\"📖PersonalGPT\")\n",
    "st.markdown(f\">:zap: Responses Powered by **{lm_model}**\")\n",
    "\n",
    "if 'is_doc_uploaded' not in st.session_state:\n",
    "    st.session_state['is_doc_uploaded'] = False\n",
    "\n",
    "\n",
    "## Add Uploader Component\n",
    "uploaded_file = st.file_uploader(\n",
    "    \"Upload a txt file\",\n",
    "    type=[\"txt\"],\n",
    "    help=\"Text files with each sentence acting as a document\",\n",
    ")\n",
    "\n",
    "if not st.session_state['is_doc_uploaded']:\n",
    "    ## Check if upload is complete\n",
    "    if not uploaded_file:\n",
    "        st.stop()\n",
    "    \n",
    "    ## Read uploaded file\n",
    "    try:\n",
    "        file_data = get_lines(uploaded_file)\n",
    "        ## Verbose Status update\n",
    "        st.markdown(f\"> Uploaded file has {len(file_data)} lines of text\")\n",
    "        st.session_state['is_doc_uploaded'] = True\n",
    "    except Exception as e:\n",
    "        st.markdown(f\"Could not upload/read file={e}\")\n",
    "        st.session_state['is_doc_uploaded'] = False\n",
    "    \n",
    "    ## Index Uploaded text file\n",
    "    with st.spinner(\"Indexing document... This may take a while⏳\"):\n",
    "        db_status_msg = load_data(db, documents=file_data)\n",
    "    \n",
    "    ## status update\n",
    "    st.markdown(f\"> Database indexed {db.count()} documents\")\n",
    "    if db.count() == 0:\n",
    "        st.markdown(db_status_msg)\n",
    "        st.session_state['is_doc_uploaded'] = False\n",
    "\n",
    "## Get User Input\n",
    "with st.form(key=\"qa_form\"):\n",
    "    query = st.text_area(\"Enter Your Query:\",\n",
    "                         placeholder=\"Examples: \\nwhat is tf-idf?\\nwhich module covers RLHF\\nhow many moons does Jupiter have?\")\n",
    "    submit = st.form_submit_button(\"Submit\")\n",
    "\n",
    "## Provide additional Options for citing source\n",
    "with st.expander(\"Advanced Options\"):\n",
    "    show_source = st.checkbox(\"Show Source\")\n",
    "\n",
    "## Generate Output upon button click\n",
    "if submit:\n",
    "  # Get relevant documents from DB\n",
    "  context = get_relevant_documents(query, db)\n",
    "\n",
    "  # get answer from LLM\n",
    "  answer,score,error = get_answer(query,context,lm_model)\n",
    "\n",
    "  # Showcase response on screen\n",
    "  st.markdown(f\"**Answer:** _{answer}_\")\n",
    "  st.markdown(f\"> **Relevance Score**:{score}\")\n",
    "  st.markdown(\"---\")\n",
    "\n",
    "  # Add more details if advanced option is chosen\n",
    "  if show_source:\n",
    "    st.markdown(\"**Source(s):**\")\n",
    "    st.markdown(f\"- <i>{context[:100]}...</i>\", unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mlGSHYN0bQSm"
   },
   "outputs": [],
   "source": [
    "# check the log file for localhost port\n",
    "# !streamlit run app.py &>logs.txt & "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5dllFNabXhE",
    "outputId": "8dd09f2d-c36d-49f1-e25d-991b18d1574c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25hnpx: installed 22 in 2.41s\n",
      "your url is: https://icy-heads-enjoy.loca.lt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# ensure setup is complete and your have install nvm/node/npm and localtunnel\n",
    "!lt --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
