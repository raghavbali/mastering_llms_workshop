{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1b1298",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop_dhs2025/blob/main/docs/module_04_llm_ops/02_dspy_demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ed0667c-fe9b-45b4-83b3-2ad4ead66225",
   "metadata": {},
   "source": [
    "# <img src=\"../assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
    "---\n",
    "<img src=\"../assets/dspy_banner.png\">\n",
    "\n",
    "---\n",
    "\n",
    "## Intro\n",
    "- Language Models are like extremely complex machines with capabilities to retrieve and reformulate information from an **extremely large latent space**.\n",
    "- To guide this search and achieve desired responses we heavily rely on **complex, long and brittle prompts** which (at times) are very specific to certain LLMs\n",
    "- Being an open area of research, teams are working from different perspectives to abstract and enable rapid development of **LLM-enabled systems**.\n",
    "- **StanfordDSpy** is one such framework for algorithmally optimizing LM prompts and weights.\n",
    "\n",
    "\n",
    "> $_{DSpy\\ logo\\ is\\ copyright/ownership\\ of\\ respective\\ teams}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39a983-e508-43a3-aaed-7c388a0be75b",
   "metadata": {},
   "source": [
    "## Ok, You Got Me Intrigued, Tell Me More?\n",
    "\n",
    "- The DSpy framework takes inspiration from deep learning frameworks such as <img src=\"./assets/pytorch_logo.png\" width=\"2%\">[PyTorch](https://pytorch.org/)\n",
    "    - For instance, to build a deep neural network using PyTorch we simply use standard layers such as ``convolution``, ``dropout``, ``linear`` and attach them to optimizers like ``Adam`` and train without worrying about implementing these from scratch everytime.\n",
    "- Similarly, DSpy provides a a set of standard general purpose **modules** (``ChainOfThought``,``Predict``), **optimizers** (``BootstrapFewShotWithRandomSearch``) and helps us build systems by composing these components as layers into a ``Program`` without explicitly dealing with prompts! Neat isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf9d1b7-8030-433a-a830-3f506cabc010",
   "metadata": {},
   "source": [
    "### Usual Prompt Based Workflow\n",
    "<img src=\"../assets/prompt_workflow.png\" width=\"75%\">\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e534227-f2b7-4acb-a7e9-91c47e33f769",
   "metadata": {},
   "source": [
    "### LangChain-Like Workflow\n",
    "\n",
    "<img src=\"../assets/langchain_workflow.png\" width=\"75%\">\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f045e-8319-433f-bbe1-ff858bcd4d73",
   "metadata": {},
   "source": [
    "### DSpy Workflow\n",
    "\n",
    "<img src=\"../assets/dspy_workflow.png\" >\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2a7ff-28f1-4cdc-a35a-0b63e7d4cb1b",
   "metadata": {},
   "source": [
    "## Time to Put Words into Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3501ddfa-233e-4b74-95a8-9da46ea63e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import dspy\n",
    "from dsp.utils import deduplicate\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from scraper_utils import NB_Markdown_Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48faf88-dabe-4c97-95d8-7ab6cd467cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_TOKEN = ''\n",
    "# lm = dspy.LM('openai/gpt-4o-mini', api_key='YOUR_OPENAI_API_KEY')\n",
    "lm = dspy.LM(\n",
    "    'ollama_chat/llama3.1',\n",
    "    api_base='http://localhost:11434',\n",
    "    api_key=OPENAI_TOKEN\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a20e56f-4061-4217-8db6-631d55ade272",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "We will scrape and extract text/markdown cells from all notebooks in this repository and prepare a dataset using the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7fbb42-fbb1-484c-8f40-932d367b80f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../module_01_lm_fundamentals',\n",
       " '../module_03_instruction_tuning_and_alignment',\n",
       " '../module_04_llm_ops',\n",
       " '../module_02_llm_building_blocks']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'../{d}' for d in os.listdir(\"../\") if d.startswith(\"module\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59bd1bea-e566-4cac-8238-42713c59dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_scraper = NB_Markdown_Scraper([f'../{d}' for d in os.listdir(\"../\") if d.startswith(\"module\")])\n",
    "nb_scraper.scrape_markdowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41c8b266-e915-46aa-bfd0-a0af44f5d64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['module_01_lm_fundamentals_01_text_representation', 'module_01_lm_fundamentals_02_contextual_embeddings', 'module_03_instruction_tuning_and_alignment_01_instruction_tuning_llama_txt2py', 'module_03_instruction_tuning_and_alignment_02_RLHF_phi2', 'module_03_instruction_tuning_and_alignment_03_zephyr_alignment_dpo', 'module_04_llm_ops_01_retrieval_augmented_llm_app', 'module_04_llm_ops_02_dspy_demo', 'module_02_llm_building_blocks_02_transformers_pipelines', 'module_02_llm_building_blocks_03_training_language_models', 'module_02_llm_building_blocks_01_transformers', 'module_02_llm_building_blocks_04_llm_training_and_scaling'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_scraper.notebook_md_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b5e711-ac90-4ece-bdd8-6905d9f2a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dspy_content.tsv\", \"w\") as record_file:\n",
    "    for k,v in nb_scraper.notebook_md_dict.items():\n",
    "        record_file.write(f\"{k}\\t{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53358b2d-7930-449e-9008-95a8f180b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = []\n",
    "ctr = 1\n",
    "for k,_ in nb_scraper.notebook_md_dict.items():\n",
    "    doc_ids.append(f'{ctr}_{k}')\n",
    "    ctr+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef35ce5-13b9-494a-82e0-16195bd12099",
   "metadata": {},
   "source": [
    "## Setup Chroma\n",
    "We started this workshop with **text representation** as one of the key components of any NLP system.\n",
    "As we progressed from simple Bag of Words setup to highly contextualised Transformer models, we now have rich & dense representations.\n",
    "The utility of such representations also increased multifold from word/sentence representations to features that can used for a number of downstream tasks.\n",
    "\n",
    "These representations, also called as vectors or embedding vectors are long series of numbers. Their retrieval and persistence requires specialised database management systems called **Vector Databases**.\n",
    "\n",
    "Vector Databases are particularly suited for handling data in the form of vectors, embeddings, or feature representations, which are commonly used in various applications like machine learning, natural language processing, computer vision, and recommendation systems.\n",
    "\n",
    "Key Features:\n",
    "- High-dimensional Data Support\n",
    "- Similarity Search\n",
    "- Indexing Techniques\n",
    "- Dimensionality Reduction\n",
    "\n",
    "There are a number of different off-the-shelf options available, such as:\n",
    "- [ChromaDB](https://www.trychroma.com/)\n",
    "- [PineCone](https://www.pinecone.io/)\n",
    "- [Milvus](https://milvus.io/)\n",
    "- [Weaviate](https://weaviate.io/)\n",
    "- [AeroSpike](https://aerospike.com/)\n",
    "- [OpenSearch](https://opensearch.org/)\n",
    "\n",
    "**Let's Install the Dependencies**\n",
    "```python\n",
    "!pip install -q chromadb\n",
    "!pip install retry\n",
    "!pip install -U sentence-transformers\n",
    "```\n",
    "\n",
    "> ensure chroma is running on your terminal `$>chroma run --path ./chromadb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65251496-7177-42a4-976c-b5314bcdc02c",
   "metadata": {},
   "source": [
    "## Vector Database: ChromaDB\n",
    "\n",
    "As mentioned above, there are a number of offering available. For this workshop we will make use of\n",
    "[ChromaDB](https://www.trychroma.com/).\n",
    "\n",
    "It is a super simple setup which is easy to use. The following figure showcases the overall flow\n",
    "\n",
    "<img src=\"../assets/04_chroma_workflow.png\">\n",
    "\n",
    "> Source :[chromadb](https://docs.trychroma.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46476bc4-0aa4-401c-bc11-951a7985b287",
   "metadata": {},
   "source": [
    "## Sentence Transformers\n",
    "\n",
    "This is an amazing python framework initially proposed along with the seminal paper titled [Sentence-BERT](https://www.sbert.net/).\n",
    "It provides clean high-level interfaces to easily use Language Models for computing text embeddings for various use-cases.\n",
    "\n",
    "In this notebook we will leverage pretrained models supported by sentence transformer rather than directly using the package.\n",
    "\n",
    "There is a [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) now maintained to keep track of the state-of-the-art embedding models called the **Massive Text Embedding Benchmark (MTEB) Leaderboard**\n",
    "\n",
    "<img src=\"../assets/04_mteb.png\">\n",
    "\n",
    "> Source : [HuggingFace](https://huggingface.co/spaces/mteb/leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a1d418-aa47-4ff2-bc75-ba1f8318095b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "chroma_emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "client = chromadb.HttpClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c70cddc6-f946-454b-b7a4-6ff5c64b82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_COLLECTION_NAME = \"workshop_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26fb8bb-0fe8-4701-84f8-d7a23fcf71f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.delete_collection(CHROMA_COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "716310d4-9f55-41f3-a6b9-f509c7680c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\n",
    "    CHROMA_COLLECTION_NAME,\n",
    "    embedding_function=chroma_emb_fn,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e95f64-c238-4a86-94b2-d3f040346d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to collection\n",
    "collection.add(\n",
    "    documents=[v for _,v in nb_scraper.notebook_md_dict.items()], \n",
    "    ids=doc_ids, # must be unique for each doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2443ccd3-7d06-4071-9657-408cba6d4fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1_module_01_lm_fundamentals_01_text_representation', '4_module_03_instruction_tuning_and_alignment_02_RLHF_phi2', '2_module_01_lm_fundamentals_02_contextual_embeddings']\n",
      "[0.799641268192753, 0.8011068851342594, 0.851910180221277]\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"RLHF\"], # Chroma will embed using the function we provided\n",
    "    n_results=3 # how many results to return\n",
    ")\n",
    "print(results['ids'][0])\n",
    "print(results['distances'][0])\n",
    "#print([i[:100] for j in results['documents'] for i in j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c036bf-6144-4945-9732-0ddebd2426f4",
   "metadata": {},
   "source": [
    "# Chroma as RM for DSPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15544929-dbd3-4598-9c60-a193be2c7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5fcb6f8-f674-44c1-ab1d-3d8c489c2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "import os\n",
    "import openai\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "475245a5-37ea-4e60-8bda-60c10f0c490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id::1_module_01_lm_fundamentals_01_text_representation\n",
      "Document score::0.799641268192753\n",
      "Document: <a target=\"_blank\" href=\"https://colab.research.go ... \n",
      "\n",
      "Document id::4_module_03_instruction_tuning_and_alignment_02_RLHF_phi2\n",
      "Document score::0.8011068851342594\n",
      "Document: <a target=\"_blank\" href=\"https://colab.research.go ... \n",
      "\n",
      "Document id::2_module_01_lm_fundamentals_02_contextual_embeddings\n",
      "Document score::0.851910180221277\n",
      "Document: <a target=\"_blank\" href=\"https://colab.research.go ... \n",
      "\n",
      "Document id::5_module_03_instruction_tuning_and_alignment_03_zephyr_alignment_dpo\n",
      "Document score::0.8664296651969585\n",
      "Document: <a target=\"_blank\" href=\"https://colab.research.go ... \n",
      "\n",
      "Document id::9_module_02_llm_building_blocks_03_training_language_models\n",
      "Document score::0.9139603656846649\n",
      "Document: <a target=\"_blank\" href=\"https://colab.research.go ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever_model = ChromadbRM(\n",
    "    CHROMA_COLLECTION_NAME,\n",
    "    './chromadb/',\n",
    "    embedding_function=chroma_emb_fn,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "results = retriever_model(\"RLHF\", k=5)\n",
    "\n",
    "for result in results:\n",
    "    print(f'Document id::{result.id}')\n",
    "    print(f'Document score::{result.score}')\n",
    "    print(\"Document:\", result.long_text[:50],'...' ,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb257f26-83be-492a-bd2f-0f2fb6803895",
   "metadata": {},
   "source": [
    "## Basic DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e669fa46-374c-400c-bd62-d29165822918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LM and RM\n",
    "dspy.settings.configure(\n",
    "    lm=lm,\n",
    "    rm=retriever_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c738473d-a729-4e7c-99a7-1736bd3ecffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c4a40f-a3c6-4cf5-8129-be079ba4e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "deb768da-94f8-4c47-81ce-191e476daab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List the models covered in module02\n",
      "Predicted Answer: GPT-2, Transformer Models\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    }
   ],
   "source": [
    "my_question = \"List the models covered in module02\"\n",
    "compiled_rag = RAG()\n",
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "pred = compiled_rag(my_question)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "for c in pred.context:\n",
    "    print(f\"Retrieved Contexts (truncated):{c[:100]}...\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529bb6a-e39f-4a07-b0f0-33c53f478f61",
   "metadata": {},
   "source": [
    "## Multi-Hop DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f7f79f4-b908-4e76-833b-4da612550ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1261053-1212-476d-81f2-6eb4329fee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateSearchQuery(dspy.Signature):\n",
    "    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    query = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b650da3-a6fa-4a9b-869b-8d07ae96013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBaleen(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_query[hop](context=context, question=question).query\n",
    "            passages = self.retrieve(query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        pred = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=pred.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f13c28c6-1e04-47dd-a6f5-25c6581c134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: List the models covered in module02\n",
      "Predicted Answer: 1. **Transformers**: A complex model built like LEGO blocks using multiple smart and specialized components.\n",
      "2. **GPT-2**: A transformer-based model that was pre-trained on a large corpus of text data to learn general language patterns and concepts.\n",
      "3. **InstructGPT**: A model that was fine-tuned on a dataset of human feedback to align its outputs with human expectations and preferences.\n",
      "4. **Phi-1.5**: A model that was used as an example in the context of retrieval augmented LLM app.\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "Retrieved Contexts (truncated):<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    }
   ],
   "source": [
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "uncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program\n",
    "pred = uncompiled_baleen(my_question)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "for c in pred.context:\n",
    "    print(f\"Retrieved Contexts (truncated):{c[:100]}...\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc408a11-7281-4e62-9eea-c230c1b8e4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-03T16:11:58.786666]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `context` (str): may contain relevant facts\n",
      "2. `question` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `answer` (str): often between 1 and 5 words\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## context ## ]]\n",
      "{context}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Answer questions with short factoid answers.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «««\n",
      "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop_dhs2025/blob/main/docs/module_04_llm_ops/02_dspy_demo.ipynb\">\n",
      "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
      "    </a> # <img src=\"../assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
      "    ---\n",
      "    <img src=\"../assets/dspy_banner.png\">\n",
      "    \n",
      "    ---\n",
      "    \n",
      "    ## Intro\n",
      "    - Language Models are like extremely complex machines with capabilities to retrieve and reformulate information from an **extremely large latent space**.\n",
      "    - To guide this search and achieve desired responses we heavily rely on **complex, long and brittle prompts** which (at times) are very specific to certain LLMs\n",
      "    - Being an open area of research, teams are working from different perspectives to abstract and enable rapid development of **LLM-enabled systems**.\n",
      "    - **StanfordDSpy** is one such framework for algorithmally optimizing LM prompts and weights.\n",
      "    \n",
      "    \n",
      "    > $_{DSpy\\ logo\\ is\\ copyright/ownership\\ of\\ respective\\ teams}$ ## Basic DSPy Program ## Multi-Hop DSPy Program ## Let Us Add Some Checks/Assertions ## Next Steps\n",
      "    \n",
      "    - Once we have assertions in place, we need to develop an evaluation metric like ``number_of_outofcontext_responses`` which can be a simple average of cases where assertion fails\n",
      "    - Prepare a golden few-shot dataset\n",
      "    - Fine-tune/distill a student model (without assertions) using a teacher model (with assertions) to improve the overall pipeline ## How Does it All Go? ## Ok, You Got Me Intrigued, Tell Me More?\n",
      "    \n",
      "    - The DSpy framework takes inspiration from deep learning frameworks such as <img src=\"./assets/pytorch_logo.png\" width=\"2%\">[PyTorch](https://pytorch.org/)\n",
      "        - For instance, to build a deep neural network using PyTorch we simply use standard layers such as ``convolution``, ``dropout``, ``linear`` and attach them to optimizers like ``Adam`` and train without worrying about implementing these from scratch everytime.\n",
      "    - Similarly, DSpy provides a a set of standard general purpose **modules** (``ChainOfThought``,``Predict``), **optimizers** (``BootstrapFewShotWithRandomSearch``) and helps us build systems by composing these components as layers into a ``Program`` without explicitly dealing with prompts! Neat isn't it? ### Usual Prompt Based Workflow\n",
      "    <img src=\"../assets/prompt_workflow.png\" width=\"75%\">\n",
      "    \n",
      "    ---\n",
      "     ### LangChain-Like Workflow\n",
      "    \n",
      "    <img src=\"../assets/langchain_workflow.png\" width=\"75%\">\n",
      "    \n",
      "    ---\n",
      "     ### DSpy Workflow\n",
      "    \n",
      "    <img src=\"../assets/dspy_workflow.png\" >\n",
      "    \n",
      "    ---\n",
      "     ## Time to Put Words into Action ## Prepare Data\n",
      "    \n",
      "    We will scrape and extract text/markdown cells from all notebooks in this repository and prepare a dataset using the same. ## Setup Chroma\n",
      "    > ensure chroma is running on your terminal `$>chroma run --path ./chromadb` # Chroma as RM for DSPY\n",
      "»»»\n",
      "[2] «««\n",
      "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop/blob/main/docs/module_02_llm_building_blocks/02_transformers_pipelines.ipynb\">\n",
      "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
      "    </a> # Transformer Task Pipelines   ## BERT-ology\n",
      "    - BERT, or __[Bi-Directional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805)__, was presented by Devlin et al., a team at Google AI in 2018\n",
      "    - Multi-task Learning: BERT also helped push the transfer-learning envelope in the NLP domain by showcasing how a pre-trained model can be fine-tuned for various tasks to provide state-of-the-art performance\n",
      "    - BERT tweaked the usual Language Model objective to only predict next token based on past context by building context from both directions, i.e. the objective of predicting masked words along with next sentence prediction.\n",
      "    \n",
      "    \n",
      "    <img src=\"../assets/02_bert_models_layout_notebook_3.jpeg\">\n",
      "    \n",
      "    > source [PLM Papers](https://github.com/thunlp/PLMpapers) ### Predicting the Masked Token\n",
      "    This was a unique objective when BERT was originally introduced as compared to usual NLP tasks such as classification. The objective requires us to prepare a dataset where we mask a certain percentage of input tokens and train the model to learn to predict those tokens. This objective turns out to be very effective in helping the model learn the nuances of language. \n",
      "    \n",
      "    In this first task we will test the pre-trained model against this objective itself. The model outputs a bunch of things such as the predicted token, encoded index of the predicted token/word along with a score which indicates the model's confidence. ### Question Answering\n",
      "    This is an interesting NLP task and quite complex one as well. For this task, the model is provided input consisting of the context along with a question and it predicts the answer by selecting text from the context. The training setup for this task is a bit involved process, the following is an overview:\n",
      "    - The training input as triplet of context, question and answer\n",
      "    - This is transformed as combined input of the form ``[CLS]question[SEP]context[SEP]`` or ``[CLS]contex[SEP]question[SEP]`` with answer acting as the label\n",
      "    - The model is trained to predict the start and end indices of the the corresponding answer for each input.\n",
      "    \n",
      "    \n",
      "    For our current setting, we will leverage both _pretrained_ and _fine-tuned_ versions of **DistilBERT** via the _question-answering_ pipeline and understand the performance difference. # Generative Pretraining ## Behold, its GPT (Generative pre-training)\n",
      "    \n",
      "    The first model in this series is called GPT, or Generative Pre-Training. It was released in [2018](https://openai.com/blog/language-unsupervised/), about the same time as the BERT model. The paper presents a task-agnostic architecture based on the ideas of transformers and unsupervised learning.\n",
      "    \n",
      "    - GPT is essentially a language model based on the __transformer-decoder__ \n",
      "    - Introduction of large training datasets: __BookCorpus__ dataset contains over 7,000 unique, unpublished books across different genres\n",
      "    - The GPT architecture makes use of 12 decoder blocks (as opposed to 6 in the original transformer) with 768-dimensional states and 12 self-attention heads each.\n",
      "    \n",
      "    \n",
      "    ### GPT-2\n",
      "    - Radford et al. presented the GPT-2 model as part of their work titled [Language Models are Unsupervised Multi-task Learners in 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
      "    - The model achieves state-of-the-art performance in a few-shot setting\n",
      "    - Similar to GPT, the secret sauce for GPT-2 is its dataset. The authors prepared a massive 40 GB dataset by crawling 45 million outbound links from a social networking site called Reddit.\n",
      "    - The vocabulary was also expanded to cover 50,000 words and the context window was expanded to 1,024 tokens (as compared to 512 for GPT).\n",
      "    \n",
      "    \n",
      "    ### GPT-3\n",
      "    - OpenAI published paper titled [Language Models are Few Shot Learners](https://arxiv.org/abs/2005.14165) in May 2020. \n",
      "    - This paper introduces the mammoth __175 billion-parameter GPT-3 model__.\n",
      "    - Apart from more layers and parameters, this model made use of sparse attention\n",
      "    - Dataset again played a key role, a 300 billion-token dataset based on existing datasets like Common Crawl (filtered for better content), WebText2 (a larger version of WebText used for GPT-2), Books1 and Books2, and the Wikipedia dataset was prepared for this model ## Language Modeling\n",
      "    By far the most widely used application from the NLP world is language modeling. We use it daily on our phone keyboards, email applications and a ton of other places.\n",
      "    \n",
      "    In simple words, a language model takes certain text as input context to generate the next set of words as output. This is interesting because a language model tries to understand the input context, the language structure (though in a very naive way) to predict the next word(s). We use it in the form of text completion utilities on search engines, chat platforms, emails etc. all the time. Language models are a perfect real life application of NLP and showcase the power of RNNs.\n",
      "    \n",
      "    Language models can be developed train in different ways. The most common and widely used method is the sliding window approach. The model takes a small window of text as input and tried to predict the next word as the output. The following figure illustrates the same visually.\n",
      "    \n",
      "    <img src=\"../assets/02_lm_training_notebook_3.png\"> ### PreTrained GPT2 for Text Generation ---\n",
      "    ## Recap\n",
      "    - **BERT and DistilBERT**: The notebook introduces BERT (Bidirectional Encoder Representations from Transformers) and its variants, explaining their unique masked language modeling objective and transfer learning capabilities. It demonstrates using pipelines to predict masked tokens and perform question answering tasks with both pre-trained and fine-tuned versions of DistilBERT.\n",
      "    - **GPT Series**: The notebook covers the evolution of GPT models (Generative Pre-Training), including GPT, GPT-2, and GPT-3. It highlights their architectures, datasets, and achievements in language modeling, emphasizing their role as unsupervised multi-task learners.\n",
      "    - **Language Modeling**: The notebook discusses the concept of language modeling, its applications in text completion, and the use of sliding window approaches for training models. It also provides an example using a pre-trained GPT2 model to generate text based on given input context.\n",
      "»»»\n",
      "[3] «««\n",
      "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop_dhs2025/blob/main/docs/module_02_llm_building_blocks/03_training_language_models.ipynb\">\n",
      "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
      "    </a> # Training Language Models ---\n",
      "    ## Recap\n",
      "    - **Training Paradigm**: The notebook introduces the two-step training paradigm for transformer models, involving pretraining on large datasets to learn general language patterns and fine-tuning on task-specific data.\n",
      "    - **Dataset Preparation and Tokenization**: It details how to prepare datasets for training, including loading datasets, tokenizing text using a tokenizer from Hugging Face, and mapping tokenized data for model input.\n",
      "    - **Model Training and Generation**: The notebook covers setting up the training environment, configuring the GPT-2 model, and running the training process. Additionally, it demonstrates code generation using the trained model with different temperature settings. ## The 2-Step Training Paradigm\n",
      "    \n",
      "    Transformers are complex models built like LEGO blocks using multiple smart and specialized components.\n",
      "    - A vanilla transformer model consists of separate stacks of encoders and decoders.\n",
      "    - Each encoder block includes multi-head self-attention, enabling the model to capture relationships between tokens regardless of their positions.\n",
      "    - Residual connections help maintain gradient flow, preventing the vanishing gradient problem.\n",
      "    - Layer normalization ensures training stability, and feed-forward layers introduce non-linearity and learn complex token interactions.\n",
      "    - Decoder blocks contain the same components but also include an encoder-decoder attention mechanism to incorporate context from the encoder.\n",
      "    - The model uses embedding layers to convert tokens into a continuous latent space for contextual learning and positional encoding to preserve the order of tokens in the sequence\n",
      "    \n",
      "    <img src=\"../assets/02_training_setup_01.png\">\n",
      "    \n",
      "    > Source: Backcock, Bali et. al.\n",
      "    \n",
      "    The two-step **training paradigm** in transformer models is designed as follows:\n",
      "    - **Pretraining** on large raw datasets like open-webtext, allowing the model to learn broad language patterns and concepts. This forms a strong foundation for various NLP tasks.\n",
      "    - The second step, **fine-tuning**, uses task-specific datasets to tailor the model to particular tasks or domains. ## Imports and Utils ## Dataset Preparation ## Tokenize ## Load Model ## Training Setup ## Push to Hub ## Let's Generate Some Code\n",
      "»»»\n",
      "[4] «««\n",
      "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop/blob/main/docs/module_03_instruction_tuning_and_alignment/02_RLHF_phi2.ipynb\">\n",
      "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
      "    </a> # Quick Overview of RLFH\n",
      "    \n",
      "    The performance of Language Models until GPT-3 was kind of amazing as-is. What the models of were essentially lacking was the aspect of **alignment**. The language generation aspect was particularly challenging due to heavy hallucinations, toxicity, etc.\n",
      "    \n",
      "    The authors of the seminal work **[InstructGPT](https://arxiv.org/pdf/2203.02155)** basically focussed on this aspect of aligning the language models to user's intructions (hence the name!). Their work showcased how we can further fine-tune such models in a supervised way leverage human feedback and reinforcement learning to align them.\n",
      "    \n",
      "    ## High-Level Overview of the Setup\n",
      "    \n",
      "    <img src=\"../assets/03_instruct_gpt_rlhf.png\">\n",
      "    \n",
      "    > Source: https://arxiv.org/pdf/2203.02155 ## Prepare Dataset ## Setup PPO Trainer ### Setup Reward Model and Utils ### Reward Assignment\n",
      "    \n",
      "    The objective is to align our text generation model towards the alignment signal provided.\n",
      "    To do so, we need to assign a corresponding reward to each output logit ## Time to Align using RLHF : PPO\n",
      "    - Get a batch of queries and prepare training input\n",
      "    - Get the query responses from the policy (model to be aligned)\n",
      "    - Join query and responses and tokenize for reward based on sentiment analysis\n",
      "    - Optimize policy with PPO using the (query, response, reward) triplet\n",
      "    - Log all the training statistics ### Plot Reward Distribution ### Compare Rewards ### Save Aligned Model Objects ## Generate and Compare Aligned vs Non-Aligned Models ## Key Concepts:\n",
      "    \n",
      "    - **Reinforcement Learning (RL)**: A machine learning paradigm where an agent learns to make decisions by performing actions and receiving _rewards or penalties_ .\n",
      "    - **Human Feedback**: Evaluations provided by humans that guide the learning process, ensuring the model's outputs align with human expectations and preferences. ## How Does this Actually Work? Show Me Examples Please?\n",
      "    \n",
      "    ### Standard Supervised Learning:\n",
      "    - Input: \"Generate a story about a dragon and a knight.\"\n",
      "    - Output: The model generates a story based on its training data.\n",
      "    \n",
      "    \n",
      "    ### Reinforcement Learning From Human Feedback:\n",
      "    - Input: \"Generate a story about a dragon and a knight.\"\n",
      "    - Initial Output: The model generates a story.\n",
      "    - Human Feedback: A human rates the story on coherence, creativity, and engagement.\n",
      "    - Adjusted Output: The model refines its story generation based on the feedback, leading to more engaging and coherent stories over time.\n",
      "    \n",
      "    As pointed out in the figure above, one of the ways of bringing this alignment is through:\n",
      "    - Training a **reward model** using a Human labelled dataset.\n",
      "        - This dataset basically contains rank ordered responses to different inputs to the model\n",
      "    - The reward model learns to predict human preferences based on the provided human feedback by assigning a score (reward) to the outputs of the language model.\n",
      "    -  The output of the reward model is then used to update the policy of the language model (agent) to align with the human feedback (exploration vs exploitation)\n",
      "    \n",
      "    \n",
      "    <img src=\"https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/trl_overview.png\">\n",
      "    \n",
      "    > Source: https://huggingface.co/docs/trl/en/quickstart ## PPO vs DPO\n",
      "    \n",
      "    There are different ways of performing policy optimisation. The original work follows Proximal Policy Optimisation which requires a separate reward model to tune the Language Model. DPO or Direct Policy Optimisation directly applies updates to the language model thus removing the need for a separate reward model.\n",
      "    \n",
      "    > Read more about RL and KL Divergence to understand the topic better\n",
      "    \n",
      "    \n",
      "    > The KL-divergence between the two outputs is used as an _additional reward signal_ to make sure the generated responses don’t deviate too far from the reference language model. # Quick Hello World Using PPO\n",
      "    \n",
      "    > Can we improve alignment of Phi-1.5?\n",
      "    \n",
      "    > Adapted from:\n",
      "    > [[1](https://huggingface.co/docs/trl/en/quickstart)], [[2](https://github.com/antndlcrx/oxford-llms-workshop/blob/main/materials/seminars/day_3/8_LLMs%20alignment%20with%20RLHF.ipynb)]\n",
      "    \n",
      "    **NOTE**: Ensure you have high-capacity GPU available for this notebook! ## Install Dependencies ## Import Packages ## Configs ## Download Models\n",
      "»»»\n",
      "[5] «««\n",
      "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop/blob/main/docs/module_04_llm_ops/01_retrieval_augmented_llm_app.ipynb\">\n",
      "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
      "    </a> # Retrieval Augmented LLM App\n",
      "    <img src=\"../assets/rap_banner.jpeg\">\n",
      "    \n",
      "    We have covered quite some ground in terms of understanding and building components for:\n",
      "    - Text Representation\n",
      "    - NLP Tasks\n",
      "    - Pretrained Models and Transfer Learning\n",
      "    - Model Fine-Tuning PEFT\n",
      "    - SFT and LLM Landscape\n",
      "    - Vector Databases\n",
      "    - Libraries and Frameworks\n",
      "    \n",
      "    Now we will work through development of an app to showcase how we can leverage all the concepts into a fully functioning system\n",
      "    \n",
      "    __Note__: In order to keep things simple, we will leverage most high-level APIs available but the overall setup should be easily extensible ## Why Retrieval Augmentation\n",
      "    \n",
      "    While theoretically LLMs are capable of having super long context windows, in real world settings this is a challenge because of:\n",
      "    - Inability/Limitation to ensure LLM focusses on correct sub-sections of the context\n",
      "    - High Memory requirements\n",
      "    - High API Cost\n",
      "    - High Latency , etc.\n",
      "    \n",
      "    \n",
      "    In order to overcome such challenges, we leverage vector databases to act as intelligent retrieval systems (again powered by LLMs) to:\n",
      "    - Provide focussed context\n",
      "    - Reduce memory, cost and latency requirements\n",
      "    - Unlock super-abilities to use upto-date information\n",
      "    - Offload trivial tasks to expert systems ## Streamlit Enters the Arena\n",
      "    \n",
      "    [Streamlit](https://streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. ## Let us Begin with Installation\n",
      "»»»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "List the models covered in module02\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "\n",
      "The models covered in module 02 are:\n",
      "\n",
      "1. Transformers\n",
      "2. GPT-2\n",
      "3. InstructGPT\n",
      "4. Phi-1.5\n",
      "\n",
      "These models were discussed in the context of training language models, instruction tuning and alignment, and retrieval augmented LLM app.\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "\n",
      "1. **Transformers**: A complex model built like LEGO blocks using multiple smart and specialized components.\n",
      "2. **GPT-2**: A transformer-based model that was pre-trained on a large corpus of text data to learn general language patterns and concepts.\n",
      "3. **InstructGPT**: A model that was fine-tuned on a dataset of human feedback to align its outputs with human expectations and preferences.\n",
      "4. **Phi-1.5**: A model that was used as an example in the context of retrieval augmented LLM app.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b16b5-d6e4-44c1-889f-370f398ebd08",
   "metadata": {},
   "source": [
    "## How Does it All Go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbff9178-1c78-4a2b-968f-aa560d8ad29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4a340b4-39f4-477e-bf26-4b9b3951b0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question**:Which model is used for instruction fine-tuning?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: The model used for instruction fine-tuning is LoRA (Low-Rank Adaptation)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:List the models covered in module03"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: The models covered in module03 are:\n",
       "1. InstructGPT\n",
       "2. Phi-1.5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:Summarize key takeaways for module02"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: Key takeaways from the module include:\n",
       "\n",
       "1. Instruction tuning is a method that enhances the ability of LLMs to generalize across diverse tasks by adding instructions to input-output pairs.\n",
       "2. Quantization and LoRA are optimization techniques used in instruction tuning to reduce computational costs while maintaining performance.\n",
       "3. LLaMA is a state-of-the-art foundational large language model designed for research purposes, with smaller models like LLaMA 2 and LLaMA 3.1 available."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:What is the focus of the following modules: module_01,module_02,module_03 and module_04? Respond as a list"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: The focus of each module is as follows:\n",
       "- Module_01: Mastering LLMs\n",
       "- Module_02: Building blocks of LLMs (training language models)\n",
       "- Module_03: Instruction tuning and alignment using RLHF"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question**:For RLHF what policy is covered in module03?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Predicted Answer**: The policy covered in module 03 for RLHF is likely the **fine-tuning** policy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Retrieved Contexts (truncated)**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n",
      "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/mastering_llms_workshop...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Which model is used for instruction fine-tuning?\",\n",
    "    \"List the models covered in module03\",\n",
    "    \"Summarize key takeaways for module02\",\n",
    "    \"What is the focus of the following modules: module_01,module_02,module_03 and module_04? Respond as a list\",\n",
    "    \"For RLHF what policy is covered in module03?\"\n",
    "]\n",
    "\n",
    "uncompiled_baleen = SimplifiedBaleen(passages_per_hop=5)  # uncompiled (i.e., zero-shot) program\n",
    "for question in questions:\n",
    "    display(Markdown(f\"**Question**:{question}\"))\n",
    "    pred = uncompiled_baleen(question)\n",
    "    display(Markdown(f\"**Predicted Answer**: {pred.answer}\"))\n",
    "    display(Markdown(\"**Retrieved Contexts (truncated)**:\"))\n",
    "    for c in pred.context:\n",
    "        print(f\"{c[:100]}...\" )\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696c37a-0f5a-435f-8e67-e1e9885c1a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
