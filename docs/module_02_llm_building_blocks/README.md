# LLM Building Blocks

This module explores the core components and building blocks of modern Large Language Models (LLMs), focusing on the transformer architecture, attention mechanisms, and practical pipelines for various NLP tasks.

## Table of Contents
- [Transformers: The Foundation of LLMs](01_transformers.ipynb)
- [Transformers Pipelines: Practical Applications](02_transformers_pipelines.ipynb)
- [Training Language Models](03_training_language_models.ipynb)
- [LLM Training and Scaling](04_llm_training_and_scaling.ipynb)

---

### Transformers: The Foundation of LLMs
[01_transformers.ipynb](01_transformers.ipynb)
- Dive into the transformer architecture, understand self-attention, positional encoding, and how these innovations enable LLMs to process language efficiently.

### Transformers Pipelines: Practical Applications
[02_transformers_pipelines.ipynb](02_transformers_pipelines.ipynb)
- Learn to use HuggingFace pipelines for classification, text generation, and more, with hands-on code examples.

### Training Language Models
[03_training_language_models.ipynb](03_training_language_models.ipynb)
- Explore the process of fine-tuning and training LLMs, including data preparation, loss functions, and evaluation metrics.

### LLM Training and Scaling
[04_llm_training_and_scaling.ipynb](04_llm_training_and_scaling.ipynb)
- Understand the challenges and solutions for scaling LLMs, including distributed training, optimization techniques, and resource management.

> _README auto-generated by GitHub Copilot on 2025-08-10_
---
